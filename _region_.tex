\message{ !name(../PEI.tex)}%
% Modelo para relat￳rio da disciplina de Projecto de Engenharia Informatica
% do MEI.
%
% Incorpora elementos impostos pelo Regulamento de Estudos Pos-Graduados da
% Universidade de Lisboa (Deliberacao 1506/2006 - Diario da Rep￺blica, 2.a s￩rie 
% - n.o 209 - 30 de Outubro de 2006)
%
\documentclass[12pt,openright,twoside]{report}
\usepackage[show]{chato-notes}
\usepackage[utf8]{inputenc}
% Quem tiver problemas com os acentos, trocar utf8 por latin1

\usepackage[portuguese,english]{babel}
\usepackage{times}
\usepackage{url}
\usepackage{graphicx}
\usepackage{mdwlist}
\usepackage[nottoc]{tocbibind}
\usepackage{csquotes}
\usepackage[table,hyperref,x11names]{xcolor}
\usepackage{array,booktabs}
\usepackage{multirow}

\usepackage{dialogue}
%To get figures and tables side by side
\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
% end figures and tables side by side

%footnote in tables 
\usepackage{threeparttable}
%end footnote in tables

% Indice remissivo
\usepackage{makeidx}
\makeindex

%quotes
\usepackage{epigraph}
\usepackage{attrib}
\usepackage{titlesec}

%\titlespacing{\subsubsection}{0pt}{0pt}{0pt}
\titleformat{\subsubsection}[runin]{\normalfont\bfseries}{\thesubsection.}{3pt}{}


\usepackage[nomain,acronym,xindy,nonumberlist]{glossaries}
\include{./tex/acronimos}
\makeglossary  

% Links
\usepackage{hyperref}

% Package para cabecalhos
\usepackage{fancyhdr}
\usepackage{lastpage}

\usepackage[font={small}]{caption}
\usepackage{subcaption}
\usepackage[sorting=none,  isbn=false,
url=false, doi=false, eprint=false]{biblatex}

\bibliography{bibliografia,web}
\fancyhf{} %
\lhead{\nouppercase {\leftmark}} %
\rhead{\nouppercase {\bf \thepage}}
\renewcommand{\headrulewidth}{0.1pt}

% Comando para inserir pagina em branco (inserida na numeracao, mas sem
% numero impresso) para quando e' preciso obrigar um capitulo a comecar
% do lado direito (pagina impar)
\newcommand{\LIMPA}{
\newpage
\mbox{}
\thispagestyle{empty}
}

% Igual, mas insere pagina com numero impresso (normalmente nao se usa)
\newcommand{\LIMPAC}{
\newpage
\mbox{}
\thispagestyle{plain}
}


%%%%%%% PERSONAL COMMANDS %%%%%%%%%%%
\newcommand{\distcontrollers}{\cite{:vn, Tootoonchian:2010vy,Koponen:2010th,Yeganeh:2012jm}}
\newcommand{\distcontrollerspaper}{\cite{Tootoonchian:2010vy, Koponen:2010th,Yeganeh:2012jm}}
%END 

\newcommand{\tbl}[2]{\begin{tabular}{#1}#2\end{tabular}}
\newcommand{\ml}[2]{#1 $\pm$ #2} 

%
% ALTERAR AQUI AS INFORMACOES RELATIVAS AO PROJECTO
%
\newcommand{\PEITITULO}{A Consistent and Fault-Tolerant Network
  Information Base for Scalable Software Defined Networks}
\newcommand{\PEIAutor}{Fábio Andrade Botelho}
\newcommand{\PEIAutorNumAluno}{41625}

%Orientador e CoOrientador *sem* titulos (e.g. Prof. Doutor)
\newcommand{\PEIOrientador}{Alysson Neves Bessani}
\newcommand{\PEICoOrientador}{Fernando Manuel Valente Ramos} %se nao se aplicar, nao importa o que aqui esteja

%Se aplicavel, o supervisor pode ter um titulo (Dr., Eng.) colocado aqui
\newcommand{\PEISupervisorInstituicao}{Nome Completo do Supervisor}  %se nao se aplicar, nao importa o que aqui esteja

\newcommand{\PEIAnoLectivo}{2012/2013}
\newcommand{\PEIAno}{2013}

% Comentar/descomentar conforme conveniente
\newcommand{\PEITIPO}{DISSERTA\c{C}\~{A}O }
%\newcommand{\PEITIPO}{PROJECTO }

% Comentar/descomentar conforme conveniente
%\newcommand{\PEIIdiomaTese}{\selectlanguage{portuguese}}
\newcommand{\PEIIdiomaTese}{\selectlanguage{english}}

% Comentar/descomentar conforme conveniente
%\newcommand{\MEIEspecializacao}{Segurança Informática}
%\newcommand{\MEIEspecializacao}{Arquitectura, Sistemas e Redes de Computadores}
%\newcommand{\MEIEspecializacao}{Interac��o e Conhecimento}
%\newcommand{\MEIEspecializacao}{Engenharia de Software }

\usepackage{ifpdf}
\ifpdf
\pdfinfo {
	/Author (\PEIAutor)
	/Title (Projecto em Segurança Informatica)
	/Subject (Segurança Informatica)
	/Keywords (state machine replication, software defined networks)
	/CreationDate (D:20100510104905)
}
\fi

\usepackage[dvips]{geometry}
\geometry{a4paper=true,portrait=true,left=3cm,right=3cm,top=2.5cm,bottom=3.5cm}

\title{\PEITITULO}
\author{\PEIAutor}
%\date{\today}

\begin{document}

\message{ !name(tex/feasibility.tex) !offset(-147) }
%\ref{sec:heimdall:datastore:functionalities}
% \ref{sec:heimdall:datastore:bft-smart}
%\ref{sec:heimdall:datastore:functionalities}
%\ref{section:background:of}
%!TEX root = ../PEI.tex
\label{sec:feasibility:apps}
\glsresetall

\todo{Online appendix}

To evaluate the feasibility of our distributed controller design we implemented a prototype of the previously described controller architecture by integrating applications from the Floodlight controller\footnote{\url{http://www.projectfloodlight.org/floodlight/}} with a data store built using a state-of-the-art state machine replication library, BFT-SMaRt~\cite{smart-tr}.
We considered three SDN applications provided with Floodlight: \emph{Learning Switch}  (a common layer 2 switch), \emph{Load   Balancer} (a round-robin load balancer) and \emph{Device Manager} (an application that tracks devices as they move around a network).
In this chapter we describe how they work and how we modify them. 
Then we expose particular workloads that they apply to the data store, as reaction of data plane events (e.g., a new flow), that are later used in our analysis of the data store performance.

\todo{move around a network? Queres mesmo falar sobre isso?}

%The applications were slightly modified but  we made an effort to avoid behavioral changes to the applications. The main change was shifting state from the controller's (volatile) memory to the data store efficiently (i.e., always trying to minimize communication). To the best of our knowledge the exposed services of the applications we changed are virtually indistinguishable from their predecessors.

%The objective of the experiments covered in this chapter  is to analyze the workloads generated by these applications to thereafter measure the performance of the data store when subject to such realistic demand caused by real applications.


Workloads are a simple trace (or log) of data store requests. They are a defined as a product of a data plane event, controller application and system global state (controller and data store).
Fig. \ref{fig:feasibility:workloads} exemplifies this definition: to the left we see a data plane event, triggered from the switch to the controller that, in turn, exchanges a specific sequence of messages with the data store (at the right) required to answer the event.
We will show different workloads for the three applications modified, that cover all the possible data plane events that cause an interaction with the data store. We also reveal the iterative process that defined our work by showing the incremental performance improvements done to applications with each of the data store functionalities described in section~\ref{sec:heimdall:datastore:functionalities}. Those functionalities are   a consequence of the study of the workloads existent in the initial integration of the applications (in their original state), to the data store. We believe this iterative process is valuable and should be documented, since it helps understanding what kind of bottlenecks and patterns are far from optimal when adapting existent centralized applications to our design.  

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.7]{pic/feasibility/workload-generic.pdf}   
  \caption[General workload]{General workload}
  \label{fig:feasibility:workloads}
\end{figure}

But, more importantly we use workloads to perform our feasibility study. 
We do it in three phases. 
First, we emulated a network environment in Mininet  --- a network emulation platform that enables a virtual network, running a real kernel, switch and application code on a single machine~\cite{Handigol:2012tg} ---  that consisted of a single switch and a least a pair of host devices.
Then \gls{icmp} requests (pings) were generated between pairs of host devices. 
The objective was to create \gls{of}  traffic (\texttt{packet-in} messages) from the ingress switch to the controller.
Then, for each \gls{of} request, the controller performs a variable, application-dependent number of read and write operations, of different sizes, in the data store (i.e., the \textit{workload}). 
In the controller  (the data store client) we record each data store interaction entirely (i.e., request and reply size, type of operation, etc.,)  associated with the data plane event that has caused it. 

Second, the collected workload traces were used to measure the performance of our distributed data store.
For this, we set up an environment in our cluster composed of four machines, three for the distributed data store\footnote{To tolerate the crash from a single controller ($f=1$) three replicas are needed, as explained in Section~\ref{sec:heimdall:datastore:bft-smart}.} and one to simulate the data store client (the controller). 
This client will concurrently replay a simulation of the recorded workload with equal payloads (i.e., equal message type and size) as well as an additional 4 byte field representing the expected reply size of the data store response. Then a simple data store server, meant to record the throughput, will reply to the client messages. We do not use our data store implementation because it was not designed for performance. The  goal of this experiment is to evaluate the performance of the middleware that  composes the bottleneck of the data store stack (BFT-SMaRt). 
From this experience we obtain the throughput and latency of processing a concurrent workloads (or concurrent data plane events).  We perform this experience for a variable of number of concurrent clients as data store clients.In practice this clients represent different threads in one controller and/or different controllers using the shared data store. 
In the third phase, we analyze the results of the previous phases. Then we try to improve on them by using the data store functionalities referred in Section \ref{sec:heimdall:datastore:functionalities}. The process then starts from the beginning.  

Each workload (with all its composing operations)  was run 50 thousand times, measuring both latency and throughput. 
We calculated averages, minimum, maximum and standard deviations in the 90, 95 and 99th percentile. 
Unless stated otherwise the values shown in this chapter are in the 90th percentile. Appendix A (available online \cite{support})  contains all this information in graphical and raw format (as well as the captured workload in textual information). The traces (i.e., data plane events and respective workloads) for each workload shown in this chapter are also available online (Appendix B~\cite{support}). 
In there the we can find: a script automating the data plane events in Mininet that generate all our recorded workloads as well as the trace (or log)  of both the data plane events and data store messages characteristics (i.e.,  workloads). 
There are also instructions to use our code base to replay all the experiences. 
Each machine in the performance test had two quad-core 2.27 GHz Intel Xeon E5520 and 32 GB of RAM memory, and they were interconnected with gigabit Ethernet. 
There were running  Ubuntu 12.04.2 LTS with  Java(TM) SE Runtime Environment (build 1.7.0\_07-b10) 64 bits.
 We used Mininet 2.0 (mininet-2.0.0-113012-amd64-ovf)\footnote{Available at \url{http://mininet.org}. We had an issue with this version and corrected it following online instructions available at \url{http://goo.gl/DQ7FQF}.}. 
The Floodlight version original forked  is available online\footnote{\url{http://goo.gl/RbBXag} commit 9b361fbb3f84629b98d99adc108cddffc606521f}. Finally we used BFT-SMaRt revision 334 \footnote{\url{http://code.google.com/p/bft-smart}} 

\todo{In Device Manager and Load Section say that the results can vary.}

\section{Learning Switch} 
\label{sec:feasibility:ls}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[ht]
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-broadcast}
                \caption{Broadcast packet.}
                \label{fig:ls:interaction:broadcast}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Unicast packet.}
                \label{fig:ls:interaction:unicast}
        \end{subfigure}
        \caption[Learning Switch workloads]{Broadcast packets trigger a write for the source address of the respective packet. Unicast packets have to additionally read the source address port location.}
        \label{fig:ls:interaction}
\end{figure}

The Learning Switch application emulates the hardware layer 2 switch forwarding process based on a table associating \gls{mac} addresses to the switch ports where they were last seen. 
In the application, for each switch  a different \gls{mac}-to-switch-port table is maintained in the data store. 
Each table is populated using the source address information (i.e., \gls{mac} and switch port)  presented in every OpenFlow \texttt{packet-in} request for the purpose of maintaining the location of devices. 
After learning this location, the controller can install rules in the switches to forward packets from a source to a destination. 
Until then, the controller must instruct the switch to \emph{flood} the packet to every port, with the exception of the ingress port (where the packet came in from).
Despite being a single-reader and single writer application (each switch table is only accessed by the controller managing the switch in question), we include it here for two reasons: (i) it benefits from the fault-tolerant property of our distribution process and (ii) it is commonly used as the single-controller benchmark application in the literature~\cite{Tootoonchian:2012uia,Erickson:2013er}. 

Fig.~\ref{fig:ls:interaction}  shows the detailed interaction between the switch, controller (Learning Switch) and data store for two possible cases. 
First (Fig.~\ref{fig:ls:interaction:broadcast}), the case for broadcast packets that require one write operation to store the switch-port  of the source address. 
Second (Fig.~\ref{fig:ls:interaction:unicast}),   the case for unicast packets, that not only stores the source information, but also reads the (possibly) known switch port  for the destination address.  

\todo{tens que falar na tabelas locais e o catano} 
%In its original state this application maintained an hash table associating a switch to another hash table
%relating  \gls{mac} and \gls{vlan} to switch ports. Both this hash tables were thread-safe (i.e., supported concurrent manipulation safely). This fits naturally in our key-value data store client implementation. The smart reader will wonder why does the original application is single-reader, single
%writer. This happens since the internal state is actually exposed through the northbound \gls{api} existent.  

It is critical, both for the centralized and fault-tolerant version, that each switch table is limited due to resource exhaustion (each table can potentially keep an entry for each host present in the network!).
For this reason the application limits a table to a fixed number of hosts (1K by default).
When this limit is reached the least recently used entries are replaced for new ones.  
This eviction policy favors inactives devices over actives ones.
Each access to the table (either a read or a write, even for existent keys) promotes the key to the top of a list making it the most recently used.
When the table is full, newly added entries replace the bottom entry of the same list ( the least referenced). Our data store allows the creation of tables with identical behaviour. 

% way better: 
%The LRU discards the least recently used items first. For this,
%whenever an entry is accessed, it moves to the top of the
%list. Whenever and entry is added to the table, but the  capacity is
%in the limit, the last entry in the access list is removed, to give room to the new one.
The \gls{lru} tables are not the only way to control the table entries. Learning Switch also applies timeouts (hard and soft --- see section~\ref{section:background:of})  to the flows installed in the data plane. When they expire, a switch triggers an \gls{of} \texttt{FLOW\_REMOVED} message (containing a source and a destination address) to the control plane which, in turn, deletes the associated entry from the data store and instructs the switch to remove the reverse entry (from destination to source address) from its table. This process is then repeated one more time.

\subsection{Broadcast Packet}
This workload is defined by  the operations performed in the data store when processing broadcast packets in a \acrfull{of} \texttt{packet-in} request (Fig.~\ref{fig:ls:interaction:broadcast}). Table~\ref{table:lsw0:broadcast} shows that for the purpose of associating the source address of the packet to the ingress switch-port where it was received, the Learning Switch application performs one write operation with a request size of 113 bytes and reply size of 1 byte (reporting success). 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
 Associate source address to ingress port & W & 113 & 1 \\ 
\end{tabular}
\caption[Workload lsw-0-broadcast operations]{Workload lsw-0-broadcast operations and sizes (in bytes).}
\label{table:lsw0:broadcast}
\end{table}

\subsection{Unicast Packet}
This workload adds an operation to the previous one, since for every unicast address we must also fetch the known switch port location of the destination address.Table \ref{table:lsw0:unicast} shows that this second operation has a weight of 36 bytes for the request payload (sent to the data store) and a 77 byte response size containing the known switch port.  
It should be noticed that the application always associates the source address to its ingress port. Although it may seem superfluous this is actually required since Learning Switches are limited in size and behave like \gls{lru} caches which replace least recently used entries first. By always writing the source to port association, even if existent, we guarantee that this device is tagged as recently used. We address 
this topic again in Section \ref{sec:ls:cache} when we discuss caching. 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply  \\ \toprule 
Associate source address to ingress port & W & 113 & 1\\
Read egress port for destination address & R & 36 & 77 \\
\end{tabular}
\caption[Workload lsw-0-unicast operations]{Workload lsw-0-unicast operations and sizes (in bytes).}
\label{table:lsw0:unicast}
\end{table}

\subsection{Optimizations}
The Learning Switch operations are simple. So there is not much we should do to improve them. 
Still, we noticed that there is an extreme overhead to the reported messages sizes given the content that is actually exchanged: a MAC address (6 byte standard); and  a switch port identifier.  
The size is justified by the fix overhead of the Java Object Serialization Stream Protocol used to transform the object values into byte arrays (as required by our data store interface). 
If we do it manually instead we can improve  significantly on the size of the messages exchanged. Table  \ref{table:lsw1:unicast} shows this improvement. 
Considering the total size of the messages request  we reduce the equivalent unicast workload (from Table \ref{table:lsw0:unicast} by 72\%. The same goes for the broadcast case (first operation of the unicast workload). 

Fig. \ref{fig:lsw:comparison} shows the results of the performance analysis made to the data store using the four workloads (the methodology is described in the beginning of this chapter). 
The reported metric for the average throughput  is Flows per second.
A flow represents a single execution of all the workload steps. 
\textbf{The average latency is taken over each Flow processing. }
The resulting values follow an exponential growth as we increase the load on the system by adding more clients. 

It was somewhat surprising that the difference in performance between the original versions (with workload names prefixed by lsw-0) and the optimized size versions (prefixed by lsw-1) is unnoticeable. In some cases lsw-1 is worst than lsw-0 due to the \textbf{statistical variance}. 
It can be that the network packet exchanged by clients and the data store (or between the data store replicas) is actually not affected (in size) by this change. 
However, we will soon verify that size optimizations are bearably unnoticeable in all our examples, except when differing in order of magnitude. 

However, we can see a significant difference between unicast and broadcast workloads caused by the number of messages. 
For the broadcast workload (1 message) we can support 20kFlows/s with a 3 ms latency.  
As for the unicast workload (2 messages) we have 12kFlows/s with the same 3ms latency. 
The natural conclusion we can take, is to think that if we merge the two messages that compose the broadcast workload into one (by using Micro Components - see section \ref{sec:heimdall:datastore:mc} \footnote{In this case since they are sequential operations we could use transactions if available in the data store}) we should obtain performance results equivalent to the broadcast workload. 
This is true, but with the introduction of caching to the Learning Switch application we will show why this is not necessary. 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Associate source address to ingress port & W & 29 & 1\\
Read egress port for destination address & R & 27 & 6 \\ \bottomrule
\end{tabular}
\caption[Workload lsw-1-unicast operations]{Workload lsw-1-unicast operations and sizes (in bytes).}
\label{table:lsw1:unicast}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{../data/reportGenerator/lsw-0-broadcastlsw-0-unicastlsw-1-broadcastlsw-1-unicasttxLatCmp.pdf}
\caption[Learning Switch workloads performance comparison]{Learning
  Switch workloads performance comparison (90th percentile). }
\label{fig:lsw:comparison}
\end{figure}

\subsection{Cache}
\label{sec:ls:cache}
%Before we delve into caching the Learning Switch tables in the controller we must further understand this application in order to understand how we have impact in its centralized and original behaviour. 
Given that Learning Switch is a single reader, single writer application, we can introduce caching mechanisms without impairing the consistency semantics. 
On the extreme the Learning Switch could contain an exact copy of everything that is present in the data store at every time. 
If so we can completely avoid the data store as long as we have the entries in cache.
First, we can avoid re-writing the source address to source port association when we already now it. 
In the original Learning Switch this re-write is not expensive (since it is local) and tags the source device  entry in the table such that the \gls{lru} table can refresh  the most  active hosts and delete the inactive ones.
But with the data store this is much more expensive (3 ms under acceptable load).
However if we perform this application locally, when the device is cached, the active host actually gets forgotten somewhere in time as newly (unknown) entries are added to the data store.  
We expect this to be ok since the host, being active, will benefit in latency a lot before actually being erased from the data store due to the newly added hosts. 
Also notice that while avoiding this write, in our cache implementation,  we verify that  it is correct (the ingress port known is the same from the new packet being processed).
Second, we can also avoid the read operation that queries for the egress port of the currently processed packet if that entry is already available in cache. 
With this improvement we no longer have to read values from the database as long as they fit in cache. And we still get consistency because when we update a value we also update the cache. 
We can also control the maximum number of entries in each cache (for each table). Then we will still benefit from cache and avoid replicating the data store.

\todo{code: add reading from the data store if not in cache. }

%The reader may think of an exception where we find stale data: when a host moves from a switch to another, the tables from the first switch will have  incorrect data and devices will be unreachable from that switch from some time. But this also happens with the centralized version also. This is why rules installed in the switches must have a idle and hard timeout set. When one of the timeouts expire the switch triggers an \texttt{FLOW\_REMOVED} messae to the controller, that in turns deletes the respective information in the data store.  This kind of problems reside on the data plane consistency side. Not on the control plane. 

%Cache can only improve on the overall analysis of the overall system (controller and data store) since the cache logic actually resides on the controller and not the data store.

\todo{Finish up with reference or explanation of why we did not do it}
%We don't improve on the workload.  
%We don't actually improve on the micro-benchmarks tested measures
%shown throughout this chapters. We do not improve simply because with
%cache we do not avoid or improve (by size reduction) any of the data
%store interactions present in table \ref{table:work:lsw1-1} (that
%shows the latest learning switch workload).  With cache we will only
%improve on the long run, since we can now avoid the two type of
%requests present in that table.

\section{Load Balancer}
\label{sec:feasibility:lb}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Load Balancer application employs a round-robin algorithm to distribute the requests addressed to a \gls{vip}. This way two consecutive requests are dispatched to two different servers. 
To understand its behaviour we will begin by the domain model used. Fig.~\ref{fig:lb-model} shows the three different entities used in the Load Balancer. For simplicity we only show the attributes relevant to our discussion.
The \gls{vip} entity represents a virtual endpoint with a specified \gls{ip}, port and protocol (ICMP, TCP or UDP) address. 
Each \gls{vip} can be associated with one or more pools of servers. Given that the distribution algorithm is round-robin, each pool has a current assigned server (\texttt{current-member} attribute). Finally, the third entity --- \emph{Member} --- represents a real server. 
In Table~\ref{table:lb:indexes} we see the different tables (hash tables in the original version of the application) used in Load Balancer.
The first three, track entities by their key attributes. An additional table (\texttt{vip-ip-to-id})  links  \glsplural{ip} to \glsplural{vip}. 

\begin{figure}[ht]
\TopFloatBoxes
\begin{floatrow}
\ffigbox{
\includegraphics[scale=0.6]{./pic/feasibility/lb-model.pdf}
}{\caption[Load Balancer domain model]{\small Simplified Load Balancer entity model.}
\label{fig:lb-model}}


\capbtabbox{
\small
\begin{tabular}{cccc}
  Name & Key & Value & \\ \toprule
vips  & vip-id  & vip   \\
pools & pool-id &  pool \\
members & member-id  & member    \\
vip-ip-to-id &  ip & vip-id   \\\midrule
\end{tabular}
}{\caption[Load Balancer key-value tables]{Load Balancer key-value tables.}\label{table:lb:indexes}}
\end{floatrow}
\end{figure}

%TODO explain ARP Requests. 
For every \gls{of} \texttt{packet-in} request, Load Balancer asserts if it is addressed at a \gls{vip}. If so, two different executions flows are possible. First, as seen in Fig.~\ref{fig:lb:interaction:arp2Vip}, when the event is caused by an \gls{arp} request --- from hosts attempting to translate a known \gls{ip} address to a \gls{mac} address (that is essential for communication) ---   the Load Balancer fetches the \texttt{proxy-mac} address attribute of an \gls{vip} used to reply to the source host. Second, if the event is caused by \gls{ip} data packets the application must: $(i)$ fetch the \gls{vip} information; $(ii)$ choose and fetch a \emph{Pool} (the current implementation chooses the first pool available); $(iii)$ rotate the \texttt{current-member} attribute of the \emph{Pool}  ( to round-robin); and $(iv)$  fetch the chosen  \emph{Member}  data. This execution is illustrated in Fig.~\ref{fig:lb:interaction:ip2Vip} where the round-robin operation aggregates steps $(ii)$ to $(iv)$.  

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-broadcast}
                \caption{ARP packet address at a VIP.}
                \label{fig:lb:interaction:arp2Vip}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-unicast}
                \caption{IP packets addressed at a VIP. }
                \label{fig:lb:interaction:ip2Vip}
        \end{subfigure}
        \caption[Load Balancer workloads]{Load Balancer workloads by events.}  
        \label{fig:lb:interaction}
\end{figure}



\subsection{ARP Request}
Table~\ref{table:lbw-0-arp-request}  shows the operations that result from a \gls{of} \texttt{packet-in} caused by an \gls{arp} request address to a \gls{vip}. 
In the first operation, Load Balancer attempts to retrieve the \texttt{vip-id} for the destination \gls{ip}. If he succeeds (reply is different than 0) then the retrieved \texttt{vip-id} is used to obtain the related \gls{vip} entity (entirely). Notice that the return size is 84 times bigger than a standard \gls{mac} address size (6 bytes). 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Get VIP id for the destination IP  & R & 104 & 8\\
Get VIP info (proxy MAC address) & R & 29 & 509 \\\bottomrule
\end{tabular}\caption[Workload lbw-0-arp-request( Arp Request to a
VIP) operations]{Workload lbw-0-arp-request( Arp Request to a VIP)
 operations and sizes (in bytes).}
\label{table:lbw-0-arp-request}
\end{table}


\subsection{Packets to a VIP}
Table~\ref{table:lbw-0-ip-to-vip} shows the detailed operations triggered by \gls{ip}  packets addressed at a \gls{vip}. 
The first two operations fetch the \gls{vip} entity associated with the destination \gls{ip} address of the packet. 
From the \gls{vip} we obtain the \texttt{pool-id} used to retrieve the \emph{Pool} (third operation)\footnote{Given that the current implementation always chooses the same  pool we could optimize this behaviour. However this will enable future improvements of the Load Balancing protocol in place.}. 
The next step is to perform the round-robin algorithm by updating the \texttt{current-member} attribute of the retrieved \emph{Pool}. 
This is done locally. 
Afterwards, the fourth operation attempts to replace the data store \emph{Pool} by the newly update one. 
If the \emph{Pool} has changed between the retrieve and replace operation this operation fails (reply equal to 0) and we must try again by fetching the \emph{Pool} one more time. In order to check if the versions have changed, the replace operation contains both the original and updated versions to be used by the data store. 
To succeed the original client version must be equal to the data store version when processing the request.
If successful (reply equal to 1) we can move on and read the chosen \emph{Member} (server) associated with the \texttt{member-id}  that has been determined by the round robin algorithm. 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
Get VIP id for the destination IP & R & 104 & 8\\
Get VIP Info (pool information) & R & 29 & 509\\
Get the chosen pool & R & 30 & 369\\
Conditional replace pool after round-robin & W & 772 & 1\\
Read the chosen Member & R & 32 & 221 \\ \bottomrule
\end{tabular}\caption[Workload lbw-0-ip-to-vip( IP packet to a VIP)
operations]{Workload lbw-0-ip-to-vip( IP packet to a VIP) operations
  and sizes (in bytes).}
\label{table:lbw-0-ip-to-vip}
\end{table}




\subsection{Optimizations}
\begin{table}[ht]
\small
\begin{tabular}{llccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & lbw-0 & lbw-1  & lbw-2 & lbw-3 & lbw-4 \\ \toprule 
%& &   \multicolumn{5}{c}{(Request, Reply)} \\midrule 
Get VIP id of destination IP  & R & (104,8) &\multirow{2}{*}{(104,509)} &  \multirow{2}{*}{(104,513)} &\multirow{2}{*}{\textbf{(62,324)}} & \multirow{2}{*}{-}    \\\cmidrule{1-2} 
Get VIP info (pool)   & R &  (29,509) & & & &   \\ \midrule 
Get the choosen pool  & R & (30,369)  &  - & (30,373) & -   & \multirow{3}{*}[-2mm]{(11,4)}  \\  \cmidrule{1-2} 
Replace pool after round-robin  & W & (772,1) & -
&\textbf{(403, 1)} &  - \\ \cmidrule{1-2}  
  Read the chosen Member &  R & (32,221) & - & (32,225) & \textbf{(44,4)} & \\\bottomrule  
\end{tabular}\caption[Load Balancer IP to VIP workload operations across
diferent implementations.]{Load Balancer  lbw-\textit{X}-ip-to-vip workload
  operations and respective sizes (in bytes) across diferent
  implementations. Bolded sizes represent significant differences
  across implementations. Sizes marked with \texttt{-} are equal to
  the previous. }\label{table:lbw:optimizations}
\end{table}

\begin{figure}[ht]
% \CenterFloatBoxes
%\TopFloatBoxes  
% \BottomFloatBoxes
\begin{floatrow}
\ffigbox{%
  \includegraphics[scale=0.4]{../data/reportGenerator/lbw-0-ip-to-viplbw-1-ip-to-viplbw-2-ip-to-viplbw-3-ip-to-viplbw-4-ip-to-viptxLatCmp.pdf}
}{\caption[Load Balancer lbw-X-ip-to-vip comparison]{Load Balancer ip-to-vip workload comparison }\label{fig:lbw-ip-to-vip:comparison}%
}
\capbtabbox{%
\small
  \begin{tabular}{lll} 
    Prefix &  Data store & Section\\\toprule
    lbw-0 & Simple Key-Value  & \ref{sec:}  \\
    lbw-1 & Cross References  & \ref{sec:} \\
    lbw-2 & Versioned Values & \ref{sec:} \\
    lbw-3 & Column Store & \ref{sec:} \\
    lbw-4 & Micro Components & \ref{sec:} \\ \bottomrule
    & &  \\ 
    & &  \\ 
    & &  \\ 
    & &  \\ 
    & &  \\ 
  \end{tabular}
}{%
  \caption[Name guide to Load Balancer workloads]{Name guide to Load
    Balancer workloads.}\label{table:lb-versions}
}
\end{floatrow}
\end{figure}

Table~\ref{table:lbw:optimizations} shows all the optimizations done to the 
workload triggered by a packet addressed at a \gls{vip}
address.
It is similar to previous workload tables but this time, we show how the operations are affected by the data store functionalities. 
This way we can verify the impact of each functionality individually. To simplify our discussion we prefix each workload with a different name: lbw-0, lbw-1,..., lbw-4. 
For reference, Table~\ref{table:lb-versions} relates prefixes to data store functionalities. 
Prefix lbw-0 refers to the initial key-value store implementation already presented (in Table \ref{table:lbw-0-ip-to-vip}). 
The message size is now grouped in tuples.
When no difference is noticed between implementations we use the \texttt{-} symbol instead of a tuple. 
Significant size changes are emphasized  in bold. 
Also notice that some operations are merged together (first two operations on workload lbw-1 and the last three in workload lbw-4).

For the first improvement (lbw-1) we eliminate the double step required to obtain a \gls{vip} (first two operations). 
This can be done with the Cross Reference functionality. For this when we create  the \texttt{vip-ip-to-id} table (consulted in the first workload operation)  we state that their values are key values of the \texttt{vips} table. Then the data store, can  in one operation  fetch the \gls{vip} for the provided \gls{ip} address. 
Next, in workload lbw-2 we drop an operation size in half, by ``upgrading'' the conditional replace (after round-robin) in the 4th line to a similar operation based on  versions number that are  provided by the data store while reading the \gls{vip} information (notice the increase by 4 bytes in the first read caused by adding the version number of the \gls{vip} to the reply).  
For the next improvement (lbw-3) we add Columns support to the data store. 
Then we can replace the existing read of a \gls{vip} (first two operations) and \emph{Members} by partially reads. 
With \gls{vip} entities we do not actually improve much because the local operations that follow the read in the
data store actually require a lot of the attributes of a \gls{vip} (it will become clear why in the next section). 
But with Members we improve by a factor of 56 in the return value because we only require reading its \gls{ip} attribute. 
Finally, we get to the most significant improvements by setting up a method in the data store equivalent to the local round robin operations that also returns the Member \gls{ip} in a single step.
This table actually does not shows the true beneficial of this optimization. 
Before it, we actually fetched and updated a \emph{Pool} in two separate steps with the conditional replace that could fail in case of concurrent updates. 
This is a potentially bottleneck under high-peak utilization of the balanced resources because this optimistic concurrency control mechanism will fail a lot when different controllers (or even the same) receive significant concurrent traffic addressed at the same \gls{vip}.  




\todo{Fazer um teste simple a mostrar que falhava era bem mais
  cientifico do que suspect} 
\todo{Tenho que explicar a figura de novo? Já expliquei como é que
  funciona na secção do Device Manager. Copy paste?} 


Fig.~\ref{lbw-ip-to-vip:comparison} show the performance results of our analysis to the different workloads. 
In it  we can see the same patterns of the previous analysis done with Learning Switch (see Section~\ref{sec:feasibility:ls}).
Again, the size reduction improvements seem to have little to no effect from workloads lbw1 to lbw-3. 
But this is more expected since the improvements actually have a smaller impact in the workload then in the case of the Learning Switch. We can also see that as before, message reduction has the greatest impact. 
\textbf{From workload lbw-0 to lbw-1 we see a minor improvement from 4.5kFlows/s with 4 ms latency to 6.1kFlows/s to 6.7 ms.}. 
Better than that, we see that with the final workload (lbw-4) we improve to 12kFlows/s  under 5ms latency. 
An improvement of more than double from the original workload, with only a 25\% increase in latency. 

\subsection{Cache}
With cache  we can maintain \gls{vip} entities locally. Then in the worst case the local \gls{vip} information can be stale.
This means that  clients that may try to reach a VIP that does not exists anymore or may have changed its \gls{ip}  address to provide another service. 
But this is something that already can happen in the strongly consistent version since when we fetch a \gls{vip} from the data store it can already be invalid by the time it arrives at the client. 
With cache this probability is greater but tunably  since with our cache implementation has a time-based interface. 
The client can set up what he believes to be an acceptable time such that the probability of this actually happening is considerably low. 


On the other hand caching a \gls{vip} can be very beneficial because the Load Balancer has a minimal impact on the controller pipeline while reading from the data store. 
one read in the data store. Even when processing a normal packet, not related to a VIP address at
all, the Load Balancer still has to find out if this is the case. This
workload, which only requires one operation (see table (First line of tables))
\ref{table:lbw-0-ip-to-vip,table:lbw-0-arp-request} but with  a 0 byte
reply) sets the minimum amount of work imposed by
the Load Balancer to the controller pipeline. 
requires this read operation. It would be great to avoid this
behaviour because we do not want to limit the pipeline to nearly 15.5kFlows/s
under 3ms latency (which is the data store performance when reading an
\gls{vip} as seen in Fig. \ref{fig:}). 


\todo{We can actually solve this by using the version number in the
  round robin mechanism. Ou então ser for mesmo uma VIP vai logo à
  base de dados fazer round robin. E na cache ficas com blooms filters
  para dizer aqueles que não são vips. Falta implementar. }


As a final note, caching is actually the reason why the fetch
operation of a \gls{vip} described in the previous section actually
does not bring a significant improvement. This is not accidental but
by choice. The reason is that \gls{vip} are actually read for two
different functions: packet to a vip and arp request to a vip. So we
believe it is simpler and more efficient to actually fetch to the
cache the union of attributes required by the two different
cases. This makes sense because the two events are not independent. An
\gls{arp} request addressed at a \gls{vip} is usually followed by an
\gls{ip} packet to the same \gls{vip}.  

The time the \gls{vip} is considered valid in cache is
configurable. To be efficiently used the user should consider both the
dynamic changes frequency done to a \gls{vip} and the time under which
they take effect, as well as the probability of successive arp and ip
packets to the same \gls{vip}

%Ideally we should also avoid the normal case of IP packets not
%addressed at a VIP. For this our cache  must understand what a empty
%value means FIXME. (use containsInCache . update to insert empty in
%cache. Then see if containsInCache AND get == null you can be certain
%the value is not a VIP), completely avoiding the going to the data store.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{./../data/reportGenerator//lbw-3-ip-to-notviptxLat.pdf}
\caption[Minimum impact of Load Balancer in the pipeline.]{Workload
  lbw-3-ip-to-notvip shows the minimal impact the Load Balancer
  applications has on the pipeline in our best implementation.}
\end{figure}

\section{Device Manager}
\label{sec:feasibility:dm}
\glsresetall

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Device Manager application tracks and stores host device information such as the switch-ports to where there attached to. 
This information --- that is retrieved from the \gls{of} packets that the controller receives --- is crucial to Floodlight’s Forwarding application. That is to say, that for each new flow, the Device Manager has the job of retrieving the known switch ports for the destination address that are later used by other applications that decide how to forward packets. 
Notice that this arrangement, excludes the Learning Switch as the  forwarding application in action.
Device Manager requires three data store tables listed in Table~\ref{table:dm:indexes}.
The first table (\texttt{devices}) keeps track of known devices created by the application.
The second (\texttt{macs}),  tracks the same devices by their \gls{mac} address and \gls{vlan} identifier pair.
Finally, a third table named \texttt{ips} links  \gls{ip} address to one or more devices.
\todo{Learn why the hell does on ip links to more than one device.}


\begin{figure}[ht]
\TopFloatBoxes
\begin{floatrow}
\ffigbox{
\includegraphics[scale=0.6]{./pic/feasibility/device-model.pdf}
}{\caption[Device class]{Simplified Device class.}
\label{fig:device-model}}

\capbtabbox{
\small
\begin{tabular}{cccc}
Name & Key & Value & \\ \toprule
devices & device-id &  device \\
macs & (MAC,VLAN)  & device-id   \\
ips  & IP & list of device-id's \\\midrule
\end{tabular}
}{\caption[Device Manager key-value tables]{Device Manager key-value tables.}
\label{table:dm:indexes}}
\end{floatrow}
\end{figure}

The Device Manager behaviour is simple. 
For every \texttt{packet-in} it analyses the data plane packet headers as well as the \gls{of} message to find out the switch port, \gls{mac} and \gls{vlan} information of the source address. 
Then it can update or create devices based on that information that is later used to uniquely identify a device. 
The update process may involve creating new \texttt{entities} in the device, that abstract \gls{ip} information activity seen from the device key interface (defined by the \gls{mac} and \gls{vlan} pair. 
When the device is well known (i.e., no new entities) then Device Manager still updates the ``last seen'' timestamp associated to the entity being processed. This information is later used to age out the inactive devices. 
We analyze and improve on two distinct workloads for this application differing in wether the application already knows the source device information (Fig. \ref{fig:dm:interaction:known}) or not ( Fig. \ref{fig:dm:interaction:unknown}). 
In the former case, the application mainly reads information from the data store in order to obtain location information. 
As for the latter case, the application must create the device information and update all the existent tables. 
Therefore, this workload generates more traffic between the controller and the data store.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/dm-unknown}
                \caption{Packet from an unknown device.}
                \label{fig:dm:interaction:unknown}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Packet from a known device.}
                \label{fig:dm:interaction:known}
        \end{subfigure}
        \caption[Device Manager workload events]{Workloads for this application heavily depend on the state of the data store. Unknown devices trigger several operations to the creation of these, while known devices only require an update of their "last seen" timestamp. No matter the case, the source and destination devices are retrieved if they exist.}
        \label{fig:dm:interaction}
\end{figure}

\subsection{Known Devices}

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Read the source device key & R & 408 & 8\\
Read the source device & R & 26 & 1444\\
Update "last seen" timestamp & W & 2942 & 0\\
Read the destination device key & R & 408 & 8\\
Read the destination device & R & 26 & 1369 \\bottomrule 
\end{tabular}
\caption[Workload dm-0-known (Known Devices) operations]{Workload
  dm-0-known (Known Devices) operations and sizes (in bytes).}
\label{table:ops:dm-0-known}
\end{table}

When devices are known to the application, a \texttt{packet-in} request
triggers the operations seen in table \ref{table:ops:dm-0-known}. The
first two operations read source and destination device
information in order to make their switch-ports available to the
Forwarding process. Additionally, the second operation (a write), 
updates the ``last seen'' timestamp of the source device.

\subsection{Unknown Source}
\small
\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
1) Read the source device key & R & 408 & 0\\
2) Get and increment the device id counter & W & 21 & 4\\
3) Put new device in device table & W & 1395 & 1\\
4) Put new device in \texttt{(MAC,VLAN)} table & W & 416 & 0\\
5) Get devices with source IP & R & 386 & 0\\
6 ) Update devices with source IP & W & 517 & 0\\
7) Read the destination device key & R & 408 & 8\\
8) Read the destination device & R & 26 & 1378 \\\bottomrule
\end{tabular}
\caption[Workload dm-0-unknown( ARP from Unknown Source)
operations]{Workload dm-0-unknown( ARP from Unknown Source) operations
  and sizes (in bytes).}
\label{table:ops:dm-0-unknown}
\end{table}


This workload is triggered in the specific case in which  the source device
is unknown and the \gls{of} message carries an \gls{arp} reply 
packet. Seing that both these  conditions are true, the application
proceeds  with 8 data store operations, described in table
\ref{table:ops:dm-0-unknown}. Their intention is to create device
information and update the three tables described  in the beginning
of this section.  

The first operation reads the  source device key. Being
that it is not known, this operation fails (notice in the table, that
the reply has a size  of zero bytes). As a result the application
proceeds with the creation of a device. For this, the
following write (second operation) atomically retrieves
and increments a device unique \texttt{id} counter. Afterwards, the third and fourth  operation
update, with the newly created device, the device and MAC/VLAN
tables respectively. Likewise, the fifth and sixth operations update
the \gls{ip} index table. Given that this index links an \gls{ip} to
several devices we are forced to first collect the set of devices in
order to update it. This \emph{read-modify} operation can
fail in case of concurrent updates. Under that case, both operations
would be repeated until they succeed. At this point, the Device Manager
is done with the creation of the device and can, finally, move to the
last two operations to fetch the destination device information. 

\subsection{Optimizations}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data/reportGenerator/dm-0-unknowndm-1-unknowndm-2-unknowndm-3-unknowndm-4-unknowntxLatCmp.pdf}
                \caption{}
                \label{fig:}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data/reportGenerator/dm-0-knowndm-1-knowndm-2-knowndm-3-knowndm-4-knowntxLatCmp.pdf}
                \caption{}
                \label{}
        \end{subfigure}
        \caption[Device Manager performance analysis]{}
        \label{fig:dm:performance}
\end{figure}

\begin{table}
\small
\begin{tabular}{lll} 
    Prefix &  Data store & Section\\\toprule
    dm-0 & Simple Key-Value  & \ref{sec:heimdall:datastore:kv}  \\
    dm-1 & Cross References  & \ref{sec:heimdall:datastore:cr} \\
    dm-2 & Versioned Values & \ref{sec:heimdall:datastore:vr} \\
    dm-3 & Column Store & \ref{sec:heimdall:datastore:cr} \\
    dm-4 & Micro Components & \ref{sec:heimdall:datastore:mc} \\ 
  \end{tabular}
  \caption[Name guide to Device Manager workloads]{Name guide to
    Device Manager workloads.}
  \label{table:names:dm}
\end{table}

\begin{table}[ht]
\small
\centering
\begin{threeparttable}
\begin{tabular}{ll ccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & dmw-0 & dmw-1  & dmw-2 & dmw-3 & dmw-4 \\ \toprule 
Get source key & R &(408, 8) & \multirow{2}{*}{(408,1274)} &
\multirow{2}{*}{(408,1278)} & \multirow{2}{*}{(486,1261)} &
\multirow{2}{*}{(28,1414)} \tnote{a} \\ \cmidrule{1-2}
Get source device & R & (26,1444) & & & & \\ \midrule
Update timestamp & W & (2942,0) & (2602,0) & \textbf{(1316,1)} & (667,1) & 
(36,0) \\ \midrule
Get destination key & R & (408,8) & \multirow{2}{*}[-1mm]{(408,1199)} &
\multirow{2}{*}[-1mm]{(408,1203)} & \multirow{2}{*}[-1mm]{(416,474)} &
\multirow{2}{*}[-1mm]{N/A} \\ \cmidrule{1-2}
Get destination device & R & (26,1369) &  &
 & & \\\bottomrule
\end{tabular}
\caption[Workload dm-0-known( Known Devices) operations]{Workload
  dm-0-known( Known Devices) operations and sizes (in bytes).}
\begin{tablenotes}
\item [a)] This operation also fetches the destination device.
\end{tablenotes}
\end{threeparttable}
\end{table}

%TODO - do not use put new device in MAC,VLAN table. This is
%confusing. 

\begin{table}[ht]
\small
\centering 
\begin{threeparttable}
\begin{tabular}{ll ccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & dmw-0 & dmw-1  & dmw-2 & dmw-3 & dmw-4 \\ \toprule 
Read source key & R & (408,0) & - & - & (486,0) & (28,201)\tnote{a}\\
Increment counter & W & (21,4) & -  & - & - & \multirow{5}{*}{(476,8)} \\
Update device table & W & (1395,1) & (1225,1)\tnote{b}  & - &
(1183,1) & \\
Update MAC  table & W & (416,0) & - & - & -
& \\
Get from IP index & R & (386,0) & - & - & - & \\
Update IP index  & W & (517,0) & - & - & - & \\
Get destination key & R & (408,8) &
\multirow{2}{*}{(408,1208)}\tnote{b} & \multirow{2}{*}{(408,1212)} &
\multirow{2}{*}{(416,474)} & \multirow{2}{*}{N/A}  \\ 
Get destination device & R & (26,1378)  &  & & \\\bottomrule
\end{tabular}
\caption[Workload dm-0-unknown( ARP from Unknown Source)
operations]{Workload dm-0-unknown( ARP from Unknown Source) operations
  and sizes (in bytes).}
\begin{tablenotes}
\item [a)] This operation also fetches the destination device.
\item [b)] Differences in sizes caused by a SERIALIZATION improvement 
\end{tablenotes}
\end{threeparttable}
\end{table}


First we  replace the two step operation required to fetch a device by
using Cross References tables (dm-1). Just as we have done in
analogous Load Balancer workload.  Next, we substitute the replace
operation in dm-1 by a version based replace. 

Columns (dm-3) we put the device. We  nearly do not improve  while
fetching the source device since the local logic requires reading
almost all the device attributes, but at least we do not incur in any
overhead associated with the column serialization logic. On the other
end the update timestamp operation has half the request size. We do
not improve more because the update timestamp updates a specific
element inside an array, so we actually have to replace the all array
since our update is limited. As for the destination device we can
reduce it nearly by a factor of three. 

Finally with micro components we can improve a lot. First we can
merge the source and destination device in only one operation (which
could also be done with transactions also). Following we can also
create a micro component to update the timestamp in the data
store. For this we only need to send the device key and new
timestamp which greatly improves the operation size.  

Also with workload dm-4-\ref{table:} we can improve by replacing
operations 2 to 6 by only operation that crates a new device. Again,
as in the known-devices workload we can fetch the source and
destination device information simultaneously. 

We can merge all operations (reading devices and creating them)
because we need to consult a local service (the topology manager) in
the controller before creating devices.  This is because we actually
have not shown other workloads for the Device Manager application
(e.g., when updating new \gls{ip} addresses). 



\subsection{Cache}
With cache we fetch known devices to the cache. Then in the known
devices workload case when we update the timestamp with the local version
number, the operation will only succeed if we have the most recent
device version.  \textbf{We expect this replace operation to succeed a lot of
time since we do not anticipate scenarios where different controllers
manipulate the same devices. But they can happen}. In practice this is
no different as the previous situation where the replace operation
could also fail (without cache). \textbf{If the devices are connected to different openflow
islands simultaneously than this is a bad idea since we will actually
have to perform one more request that the normal workload
pattern. (try to updated - fails, retrieve new , update) . Off course
this could be mitigated by having the update attempt to return the
currently present value timestamp}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../PEI"
%%% End: 


\message{ !name(../PEI.tex) !offset(-637) }

\end{document}
