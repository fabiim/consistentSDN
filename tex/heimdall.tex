
\section{Shared Data Store Controller Architecture}
\glsresetall
\label{sec:heimdall:architecture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The proposed distributed control architecture is based on a set of controllers acting as clients of the fault-tolerant replicated key-value data store, reading and updating the required state as the control application demands, maintaining thus only soft state locally.
There are two main concerns around this design: (i) how to avoid the storage being a single point of failure and (ii) how to avoid making the storage a bottleneck for the system.
In the previous section we showed that state-of-the-art state machine replication can be used to build a data store that solves both these concerns.

Fig. \ref{fig:architecture} shows the architecture of our shared data store distributed controller.
The architecture comprises a set of SDN controllers connected to the switches in the network.
All decisions of the control plane applications running on the distributed controller are based on OpenFlow events triggered by the switches and the consistent network state the controllers share on the data store.
The fact that we have a consistent data store makes the interaction between controllers as simple as reading and writing on the shared data store: there is no need for code that deals with conflict resolution or the complexities due to possible corner cases arising from weak consistency.

By design, the SMR-based data store is replicated and fault-tolerant (as in all designs discussed in the previous section), being up and running as long as a majority of replicas is alive~\cite{Lam98}.
In other words, $2f+1$ replicas are needed to tolerate $f$ simultaneous crashes.
Thus, besides offering strong consistency, this architecture leads to a completely fault-tolerant control plane.
Furthermore, in this design the controllers keep only soft state locally, which can be easily reconstructed after a crash.
The switches tolerate controller crashes using the master-slave configuration introduced in OpenFlow 1.2\,\cite{ONF2011}, which allows each switch to report events to up to $f+1$ controllers (being $f$ an upper bound on the number of faults tolerated), with a single one being master for each particular switch.
The master is constantly being monitored by the remaining $f$ controllers, which can takeover its role in case of a crash.

Interestingly, our architecture could also be used in SDN deployments were a distributed controller is not necessary, to implement fault tolerance for centralized controllers.
In this case the fault-tolerant data store can be used to store the pertinent controller state, making it extremely simple to recover from its crash.
In this case, the applications deployed on the primary controller manage the network while a set of $f$ backup controllers keep monitoring this primary, just as in the distributed controller design.
If the primary fails, one of the backups -- say, the one with the highest IP address -- takes the role of primary and uses the data store to continue controlling the network.

Our distributed controller architecture covers the two most complex fault domains in an SDN, as introduced in~\cite{kim2012}.
It has the potential to tolerate faults in the controller (if the controller itself or associated machinery fails) by having the state stored in the fault-tolerant data store.
It can also deal with faults in the control plane (the connection controller-switch) by having each switch connected to several controllers (which is ongoing work).
The third SDN fault domain --- the data plane --- is orthogonal to this work since it depends on the topology of the network and how control applications react to faults.
This problem is being addressed in other recent efforts~\cite{kim2012,Reitblatt2013}.

\begin{figure}
\centering
\includegraphics[scale=0.6]{./pic/heimdall/multicontroller.pdf}
%add desired spacing between images, e. g. ~, \quad, \qquad etc.
%(or a blank line to force the subfigure onto a new line) 
\caption[Heimdall Architecture]{The shared data store controller
  architecture with each switch sending OpenFlow messages to two
  controllers. The controllers coordinate their actions using a
  logically centralized data store, implemented as a set of
  synchronized replicas. }
\label{fig:architecture} 
\end{figure}

\section{Floodlight} 
\glsresetall
\label{sec:heimdall:floodlight}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Store}
\glsresetall
\label{sec:heimdall:dataStore}
%Features  design, etc., 
\label{sec:heimdall:datastore:bft-smart}
FIXME : Alysson - será que posso justificar que a data store
performance nao é importante quando comparada com o middleware? 

\subsection{Smart}
we had to change the library used of netty from netty-3.1.1.GA.jar to
netty-3.2.6.Final.jar because it conflicted with floodlight. We are in
the dark here. Do not known if this will cause problems... 

\subsection{Implementation}

\subsubsection{Map interface} 
Actually motivated by the LearningSwitch application. The rest interface exposed the learning switch database (hash table ) directly. We implemented a class to Delegate the map interface methods to our KeyValueTable. We don't actually use it inside the applications we modified because we have some preference for static typing (which for arguably legitimate reasons does not appear in the Map interface). Nevertheless we could use it. putAll and containsValue are not implemented. No special reason just lazinesss.  

LinkedHashMap should be used for maintaining consistent ordering
across replicas. 

\section{Something}
\label{sec:heimdall:datastore:functionalities}

There are 5 functionalities that we have build after integrating
Floodlight existent applications with our simple Key Value Store. 
Most of those functionalities can be found in off-the-shelf data stores (either SQL or NoSql). 

\subsubsection{Cross References}
Cross Reference tables are very similar to the secondary index functionality found in almost all SQL and  NoSQL  based data stores. 
With it we can fetch a value in the data store using \BigO{1} (considering all the entries in a table) using any of the existent indexes. 
We integrate Cross References tables since we typically found in memory based applications (\emph{Load Balancer} and \emph{Device Manager} and \emph{Topology Manager} )) that kept different hash tables in order to track the same value with different keys. 
For example \emph{Device Manager}  keeps two tables for tracking devices by their  unique sequential identifier and a \gls{mac} address (which is also unique to a device). 
This way values can be quickly found through different types of keys depending on the context causing the search. 
If we were to use a single table to keep devices then searching all the devices to find the correspondant device to \gls{mac} then we would have $\Omega(\log n)$ for $n$ devices. 

\subsubsection {Versioned}
Another common functionality is concurrency control based on version numbers that are associated with an entry in a key value table. 
On every update operation the data store increases this value by one.
With this mechanism we enable a simple concurrent interface based on version numbers that allows different clients of the data store to manipulate it safely. 
This mechanism is more commonly used in situations where we read a value, transform it and then update it. In order to not risk loosing the updates done by a concurrent client, we can use the version number of the original value when updating. The data store then can verify if the version is the least of the value or not. If not the update will fail, and we can proceed again.  

We found examples of applications (Load Balancer and Device Manager) that used this simple concurrency control primitives as defined by the Java \emph{ConcurrentMap} interface. However this interface is based on values instead of version numbers. This requires clients to send two values: one that should be equal to the data store version (only then will the operation succeed)  ; and the new one. 
In Java the concurrent hash table interfaces requires sending both the expected value and the new value. 
However with remote data stores this is not practical for two reasons. 
First, it incurs in a significant overhead since we have to send two values instead of one.
Second, it can be painfully to make sure that byte array representations of values are actually equal. When marshalling a value (transform an object into a linear byte array representation) the process in place can output different values which are logically equivalent (as specified by their equals contract in the Java case).  This can make a list that contains the same objects to have different representations which is something hard to identify and correct since we have to verify every suspected attribute of a class. 

So all in all the usage of replace based on byte comparison is not advocated. Instead timestamps end up benefiting the user by being more space efficient in the message exchanged (with a possibly insignificant cost for carrying the timestamp value); and by being easier to work with. 

\subsubsection{Columns}
Values can be composed of different attributes causing their space usage to be huge. In simple tests that we have performed we saw devices instances that would go up to 2kB. It may not seem much but remember that our middleware could support 20kOps/s with 1Kb and only 4.7kOps/s with 4kOps/s. 

So it pays offs to be economical in sizes. With column enabled data stores we do this by selectively reading one or more object attributes. 
The columns that compose an object are not static, but defined by clients. Clients can add and delete columns from a single data store value  at any time (in one table, different values may have different columns). There are no empty columns.  

The basic idea is to minimize the size of messages by being able to selectiveley access the sub elements of data store values that are constantly accessed on a per flow basis. 
As an example (elaborated on the Evaluation section) Load Balancer requires reading an \gls{ip} address for every \gls{of} addressed at a balanced resource. By using an columnar approach we were able to improve from 224 bytes to 4 . So with columns we can reduce communication  with the data store to the very minimal. 

\subsubsection{Cache}
With cache we can have a fine graine control over the staleness of data. 
This means that we can choose if we want consistency or not.
And if we choose the latter, we can define the window of inconsistency accepted. 

The cache implementation is simple. 
Every time a value is saved to  cache (either by fetching it from the data store or by updating it in the data store) we associate the local time with that value. 
Then the data store client can fetch a value in cache by specifying the accepted staleness of that value (e.g., 200 ms , 10 ms, etc., ). 
Cross References values are also kept in cache as well as column values. But with column values we  keep partially completed objects in cache. For example a client of the data store that requests the device \gls{mac} address columns get a Device in return.  That Device only has the \gls{mac} attribute set. 

\subsubsection{Micro Components}
A micro component its a magic method that the client can invoke on the data store which can make anything. Right now we copy/pasted the code of the controller to the data store. 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../PEI"
%%% End: 
