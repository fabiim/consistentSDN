

%!TEX root = ../PEI.tex
\label{sec:feasibility:apps}
\glsresetall

To evaluate the feasibility of our distributed controller design we implemented a prototype of the previously described controller architecture by integrating applications from the Floodlight controller~\footnote{\url{http://www.projectfloodlight.org/floodlight/}} with a data store built using a state-of-the-art state machine replication library, BFT-SMaRt~\cite{smart-tr,bft-smart:2011:High-perfomance}.
We considered three SDN applications provided with Floodlight: \emph{Learning Switch}  (a common layer 2 switch), \emph{Load   Balancer} (a round-robin load balancer) and \emph{Device Manager} (an application that tracks devices as they move around a network).
In this chapter we describe how they work and how we modify them. 
Then we expose particular workloads that they apply to the data store, as reaction of data plane events (e.g., a new flow), that are later used in our analysis of the data store performance.

\todo{move around a network? Queres mesmo falar sobre isso?}

%The applications were slightly modified but  we made an effort to avoid behavioral changes to the applications. The main change was shifting state from the controller's (volatile) memory to the data store efficiently (i.e., always trying to minimize communication). To the best of our knowledge the exposed services of the applications we changed are virtually indistinguishable from their predecessors.

%The objective of the experiments covered in this chapter  is to analyze the workloads generated by these applications to thereafter measure the performance of the data store when subject to such realistic demand caused by real applications.


Workloads are a simple trace (or log) of data store requests. They are a defined as a product of a data plane event, controller application and system global state (controller and data store).
Fig. \ref{fig:feasibility:workloads} exemplifies this definition: to the left we see a data plane event, triggered from the switch to the controller that, in turn, exchanges a specific sequence of messages with the data store (at the right) required to answer the event.
We will show different workloads for the three applications modified, that cover all the possible data plane events that cause an interaction with the data store. We also reveal the iterative process that defined our work by showing the incremental performance improvements done to applications with each of the data store functionalities described in section~\ref{sec:heimdall:datastore:functionalities}. Those functionalities are   a consequence of the study of the workloads existent in the initial integration of the applications (in their original state), to the data store. We believe this iterative process is valuable and should be documented, since it helps understanding what kind of bottlenecks and patterns are far from optimal when adapting existent centralized applications to our design.  

But, more importantly we use workloads to perform our feasibility study. 
We do it in three phases. 
First, we emulated a network environment in Mininet  --- a network emulation platform that enables a virtual network, running a real kernel, switch and application code on a single machine~\cite{Handigol:2012t} ---  that consisted of a single switch and a least a pair of host devices.
Then \gls{icmp} requests (pings) were generated between pairs of host devices. 
The objective was to create \gls{of}  traffic (\texttt{packet-in} messages) from the ingress switch to the controller.
Then, for each \gls{of} request, the controller performs a variable, application-dependent number of read and write operations, of different sizes, in the data store (i.e., the \textit{workload}). 
In the controller  (the data store client) we record each data store interaction entirely (i.e., request and reply size, type of operation, etc.,)  associated with the data plane event that has caused it. 
Second, the collected workload traces were used to measure the performance of our distributed data store.
For this, we set up an environment in our cluster composed of four machines, three for the distributed data store\footnote{To tolerate the crash from a single controller ($f=1$) three replicas are needed, as explained in Section \ref{sec:heimdall:datastore:bft-smart}.} and one to simulate the data store client (the controller). 
This client will concurrently replay a simulation of the recorded workload with equal payloads (i.e., equal message type and size) as well as an additional 4 byte field representing the expected reply size of the data store response. Then a simple data store server, meant to record the throughput, will reply to the client messages. We do not use our data store implementation because it was not designed for performance. The  goal of this experiment is to evaluate the performance of the middleware that  composes the bottleneck of the data store stack (BFT-SMaRt). 
From this experience we obtain the throughput and latency of processing a concurrent workloads (or concurrent data plane events).  We perform this experience for a variable of number of concurrent clients as data store clients.In practice this clients represent different threads in one controller and/or different controllers using the shared data store. 
In the third phase, we analyze the results of the previous phases. Then we try to improve on them by using the data store functionalities referred in Section \ref{sec:heimdall:datastore:functionalities}. The process then starts from the beginning. 

Each workload (with all its composing operations)  was run 50 thousand times, measuring both latency and throughput. 
We calculated averages, minimum, maximum and standard deviations in the 90, 95 and 99th percentile. 
Unless stated otherwise the values shown in this chapter are in the 90th percentile. Appendix B \cite{support}  contains all this information in graphical and raw format (as well as the captured workload in textual information). The traces (i.e., data plane events and respective workloads) for each workload shown in this chapter are available online (\cite{support} appendix A). 
In there the we can find: a script automating the data plane events in Mininet that generate all our recorded workloads as well as the trace (or log)  of both the data plane events and data store messages characteristics (i.e.,  workloads). 
There are also instructions to use our code base to replay all the experiences. 
Each machine in the performance test had two quad-core 2.27 GHz Intel Xeon E5520 and 32 GB of RAM memory, and they were interconnected with gigabit Ethernet. 
There were running  Ubuntu 12.04.2 LTS with  Java(TM) SE Runtime Environment (build 1.7.0\_07-b10) 64 bits.
 We used Mininet 2.0 (mininet-2.0.0-113012-amd64-ovf)\footnote{Available at \url{http://mininet.org}. We had an issue with this version and corrected it following online instructions available at \url{http://goo.gl/DQ7FQF}.}. 
The Floodlight version original forked  is available online\footnote{\url{http://goo.gl/RbBXag} commit 9b361fbb3f84629b98d99adc108cddffc606521f}. Finally we used BFT-SMaRt revision \textbf{??} \footnote{\url{http://code.google.com/p/bft-smart} revision \textbf{??????}} 

\todo{check smart revision.}
\todo{In Device Manager and Load Section say that the results can vary.}

\section{Learning Switch} 
\label{sec:feasibility:ls}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[ht]

  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-broadcast}
                \caption{Broadcast packet.}
                \label{fig:ls:interaction:broadcast}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Unicast packet.}
                \label{fig:ls:interaction:unicast}
        \end{subfigure}
        \caption[Learning Switch workloads]{Broadcast packets trigger a write for the source address of the respective packet. Unicast packets have to additionally read the source address port location.}
        \label{fig:ls:interaction}
\end{figure}

The Learning Switch application emulates the hardware layer 2 switch forwarding process where each switch keeps a forwarding table associating \glsplural{mac} addresses to the switch ports where they were last seen. HERE For each switch a different \emph{\texttt{MAC}-to-switch-port} table is maintained in the data store. 
Each table is populated using the source address information (i.e., \gls{mac} and switch port)  presented in every OpenFlow \texttt{packet-in} request for the purpose of maintaining the location of devices. 
After learning this location, the controller can install rules in the switches to forward packets from a source to a destination. 
Until then, the controller must instruct the switch to \emph{flood} the packet to every port, with the exception of the ingress port (where the packet came in from).
Despite being a single-reader and single writer application (each switch table is only accessed by the controller managing the switch in question), we include it here for two reasons: (i) it benefits from the fault-tolerant property of our distribution process and (ii) it is commonly used as the single-controller benchmark application in the literature~\cite{Tootoonchian:2012uia,beacon2013}. 

Fig. \ref{fig:ls:interaction}  shows the detailed interaction between the switch, controller (Learning Switch) and data store for two possible cases. 
First (figure \ref{fig:ls:interaction:broadcast}), the case for broadcast packets that require one write operation to store the switch-port  of the source address. 
Second (figure \ref{fig:ls:interaction:unicast}),   the case for unicast packets, that not only stores source information, but also reads the switch egress port for the destination address.  

In its original state this application maintained an hash table
associating a switch to another hash table relating  \gls{mac} and
\gls{vlan} to switch ports. Both this hash tables were thread-safe
(i.e., supported concurrent manipulation safely). This fits naturally
in our key-value data store client implementation. The smart reader 
will wonder why does the original application is single-reader, single
writer. This happens since the internal state is actually exposed
through the northbound \gls{api} existent.  

Also, it is critical that the switch table is limited due to resource
exhaustion. Remember that each table can potentially keep an entry for
each host present in the network. For this reason the application
limits each table to a fixed number of hosts (1K by default). Whenever
this number is reached, newly added devices replace existent
devices in the table. The eviction policy used, actually uses a
\gls{lru} eviction policy where inactives entries are the first to be
evicted. On each access to the table (get and write, even if the entry
already exists), the entry associated moves to the top of a list,
making it the most recently used. Then on eviction, we only have to replace the
bottom entry by a new one. We mimicked this behaviour in the data
store but in section \ref{sec:feasibility:lsw:cache} ahead, we explain
how we affected it. 

% way better: 
%The LRU discards the least recently used items first. For this,
%whenever an entry is accessed, it moves to the top of the
%list. Whenever and entry is added to the table, but the  capacity is
%in the limit, the last entry in the access list is removed, to give room to the new one.
But the \gls{lru} is not the only way to control the entries present
in each switch table. Learning Switch also applies timeouts (hard and
soft --- see section \ref{section:background:of})  to the flows
installed in the data plane. When they expire, a switch triggers an
\gls{of} \texttt{FLOW\_REMOVED} message (containing a source and a
destination address)to the control plane which, in
turn, deletes the associated entry from the data store and instructs
the switch to remove the reverse entry (from destination to source
address) from its table. This process can then be repeated one more
time. 

\subsection{Broadcast Packet}
Figure \ref{fig:ls:interaction:broadcast}
This workload corresponds to the operations performed in the data
store when processing broadcast packets in a \acrfull{of} 
\texttt{packet-in} request.  Table \ref{table:lsw0:broadcast} shows that for the
purpose of associating the source address of the packet to the ingress
switch-port where it was received, the Learning Switch application performs one
write operation with a request size of 113 bytes and reply size of 1
byte. 

\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
 Associate source address to ingress port & W & 113 & 1 \\ \bottomrule
\end{tabular}
\caption[Workload lsw-0-broadcast( Broadcast Packet) operations]{Workload lsw-0-broadcast( Broadcast Packet) operations and sizes (in bytes).}
\label{table:lsw0:broadcast}
\end{table}
\subsection{Unicast Packet}
Figure                \ref{fig:ls:interaction:unicast}
Workload \textbf{lsw1-1} is the result of processing an \acrfull{of} request
triggered by an unicast packet. This workload builds on the previous
one, showing an additional operation to query the switch-port location of the
destination address. Table \ref{table:lsw0:unicast} provides a summary all the data
store operations in this workload. 

\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply  \\ \toprule 
 Associate source address to ingress port & W & 113 & 1\\\midrule
Read egress port for destination address & R & 36 & 77 \\\bottomrule
\end{tabular}
\caption[Workload lsw-0-unicast( Unicast Packet) operations]{Workload lsw-0-unicast( Unicast Packet) operations and sizes (in bytes).}
\label{table:lsw0:unicast}
\end{table}

\subsection{Optimizations}
The Learning Switch workloads are very simple. So there is
not much we can do to improve them. Still, we noticed that there were
some extreme overhead to the messages sizes given the content that is
actually exchanged between the data store: a MAC address (8 byte
standard); and  a switch port identifier. The reason is simple, we
were the standard Java serialization process to
transform objects into byte arrays (used in the data store), and
vice-versa. This incurs in some standard overhead required by the Java
process. If we do it manually we can improve a lot on the size of the
messages exchanged as seen in table
\ref{table:lsw1:unicast}. Considering the total size of the messages
(request $+$ reply) we have actually reduced the equivalent unicast workload (table
\ref{table:lsw0:unicast} by 72\%. The same goes for the broadcast,
workload (the first message of table \ref{table:lsw-1-unicast}. 

Figure \ref{fig:lsw:comparison}  shows the results of the performance
analysis made to the four workloads. Each workload  curve has different points
taken from tests with increasingly client numbers. Accurate values and standard
deviations, as well as measures in the 95th and 99th percentile, can
be seen online \ref{support} (appendix ?).  It was somewhat surprising
that the difference in the performance between the original versions
(prefixed by lsw-0) and the optimized size versions (prefixed by
lsw-1) is unnoticeable (in some cases lsw-1 is worst than lsw-0 due to
the statistical variance). It can be that the network packet exchanged by
client and the data store (or between the data store replicas) is
actually not affected (in size) by this change. However, we will soon
verify that size optimizations are bearably unnoticeable in all our examples, except
when differing in order of magnitude. 

But we can see a significant difference between unicast and broadcast
workloads due to the fact that the former requires one more message
than the former. For the broadcast workload we can support 20kFlows/s
with a 3 ms latency. For the broadcast workload we have 12kFlows/s
with the same 3ms latency. 

The natural conclusion we can take, is to think that if we merge the
two messages that compose the broadcast workload into one (by using
Micro Components - see section \ref{sec:heimdall:datastore:mc}) we
should obtain performance results equivalent to the broadcast
workload. This is true, considering that if we add up the
lsw-1-unicast message sizes we get a very similar workload to
lsw-1-broadcast. However we do not do this, since that with cache, we
can potentially obtain much better results without requiring this. 

\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
Associate source address to ingress port & W & 29 & 1\\\midrule
Read egress port for destination address & R & 27 & 6 \\\bottomrule
\end{tabular}
\caption[Workload lsw-1-unicast( Unicast Packet) operations]{Workload lsw-1-unicast( Unicast Packet) operations and sizes (in bytes).}
\label{table:lsw1:unicast}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{../data/reportGenerator/lsw-0-broadcastlsw-0-unicastlsw-1-broadcastlsw-1-unicasttxLatCmp.pdf}
\caption[Learning Switch workloads performance comparison]{Learning
  Switch workloads performance comparison (90th percentile). }
\label{fig:lsw:comparison}
\end{figure}


\subsection{Cache}
\label{sec.learning.switch.lru.cache} 
\label{sec:feasibility:ls:cache}

Given that Learning Switch is a single reader, single writer
application, we can introduce caching mechanisms without impairing the
consistency semantics. To all effects, the Learning Switch can contain
an exact copy of everything that is present in the data store at every time.

As so we can completely avoid the data store as long as we have
entries in cache.
First we can avoid re-writing the source address to source port association when we already now it.
in the original Learning Switch this re-write is not costly
($\Omega(1)$) and has the functional impact of refreshing the entry
timestamp such that the least recently used table can keep up
consistently with the last active host and delete the inactive ones
(that may have moved or disconnect). But with the data store this is
much more expensive. 
Now, the active host
actually gets forgotten somewhere in time as newly (unknown) entries
are added to the data store, we expect this to be ok since the host,
being active, will benefit in latency a lot before actually being
erased from the data store due to the newly added hosts. 
When avoiding this write in the cache implementation we must actually be sure that we only
avoid to write to the data store when the association is known in
cache and it is actually correct (the ingress port is the same from
the packet being processed). 
The second avoidance is the read operation that queries for the egress
port of the current processed packet. We do not actually need to read
from the data store if the entry is present in the cache. First the
data is not modified by any other controller since we are the only
ones which manipulate our switch tables. 

With this improvement we no longer have to read values from the
database. We do not need since when we update the data store  (with
writes) we also update the cache. So if it isn't on the cache it is not in the
data store. This means that we can avoid our cache timed interface
(see section \ref{sec:heimdall:cache}) and just read from the data
store every time. Off course this means that the cache must be able to
handle the same state size that the data store (which may be
infeasible). 

The reader may think of an exception where we find stale data:
when a host moves from a switch to another, the tables from the first
switch will have  incorrect data and devices will be unreachable from
that switch from some time. But this also happens with the centralized
version also. This is why rules installed in the switches must have a
idle and hard timeout set. When one of the timeouts expire the switch
triggers an \texttt{FLOW\_REMOVED} message to the controller, that in
turns deletes the respective information in the data store.  This kind
of problems reside on the data plane consistency side. Not on the
control plane. 




Cache can only improve on the overall analysis of the overall system
(controller and data store) since the cache logic actually resides on the
controller and not the data store.

\todo{Finish up with reference or explanation of why we did not do it}

%We don't improve on the workload.  
%We don't actually improve on the micro-benchmarks tested measures
%shown throughout this chapters. We do not improve simply because with
%cache we do not avoid or improve (by size reduction) any of the data
%store interactions present in table \ref{table:work:lsw1-1} (that
%shows the latest learning switch workload).  With cache we will only
%improve on the long run, since we can now avoid the two type of
%requests present in that table.


\section{Load Balancer}
\label{sec:feasibility:lb}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Load Balancer main idea} 
The Load Balancer application employs a round-robin algorithm to distribute the
requests addressed to a \gls{vip} . 
In order to understand its behaviour we will begin by the data model currently used. Figure
\ref{fig:lb-model} shows the three different entities used in the Load
Balancer. The \gls{vip} entity represents a virtual endpoint with a specified \gls{ip}, port and
protocol address. Each \gls{vip} can be associated with one or more pools of 
servers. Given that the distribution algorithm is round-robin, each pool
has a current assigned server (\texttt{current-member} attribute in the figure). Finally, the third entity --- Member
--- represents a real server. 
Each of those entities, corresponds
to a different  table  in the data store, indexed by the entity
key attributes represented in the figure (in bold). Moreover, a fourth
table, named \texttt{vip-ip-to-id} is required to associate \gls{ip} addresses to \gls{vip} resources. 



\begin{figure}[ht]
\TopFloatBoxes
\begin{floatrow}
\ffigbox{
\includegraphics[scale=0.6]{./pic/feasibility/lb-model.pdf}
}{\caption{\small Simplified Load Balancer entities data model. The data
store contains a table for each entity, indexed by their keys (represent as bold attributes). }
\label{fig:lb-model}}


\capbtabbox{
\small
\begin{tabular}{cccc}
  Name & Key & Value & \\ \toprule
vips  & vip-id  & vip   \\\midrule
pools & pool-id &  pool \\\midrule
members & member-id  & member    \\\midrule
vip-ip-to-id &  ip & vip-id   \\\midrule
\end{tabular}
}{\caption[Load Balancer key-value tables]{Load Balancer key-value tables.}
\label{tablle:lb:indexes}}
\end{floatrow}
\end{figure}

In light of this data model, the load balancer logic requires the following
operations from the data store: (i) check if the source address is
associated with a VIP resource; (ii) if so, read the VIP, Pool and
Member information required to install flows in the switch and (iii)
update the pool \texttt{current-member} attribute. This description
corresponds to the case where \gls{of} \texttt{packet-in} requests are indeed addressed at a \gls{vip}
resource. The respective workload which, is the heavier in
the Load Balancer application, is represented in Fig. \ref{fig:lb:interaction:ip2Vip}. Alternatively, Fig. 
\ref{fig:lb:interaction:arp2VIp}  considers the special case of ARP requests questioning the hardware
address of a \gls{vip} \texttt{IP}. In the following sections we
expand on the details and improvements related to both those
workloads. 

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-broadcast}
                \caption{ARP packet address at a VIP.}
                \label{fig:lb:interaction:arp2Vip}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-unicast}
                \caption{IP packets addressed at a VIP. }
                \label{fig:lb:interaction:ip2Vip}
        \end{subfigure}
        \caption[Load Balancer workload events]{A \texttt{\gls{arp}} request message addressed at a VIP \gls{ip} that results in a direct \gls{arp} reply. On the left a normal \gls{ip} packet addressed at VIP should be resolved (who is responsible) and replied by installing the appropriate rules}  
        \label{fig:lb:interaction}
\end{figure}

\subsection{Packets to a VIP}
When the Load Balancer receives a data packet addressed
at a \gls{vip}, it triggers the operations seen in table \ref{table:lbw-0-ip-to-vip}. 
The first two operations fetch a \gls{vip} resource associated with the
destination \gls{ip} address of the packet. The first fetches the
\gls{vip} unique identifier associated with the destination
\gls{ip}. If it succeeds, the reply is different from 0 and it can
proceed to the second operation where the \gls{vip} entity is fetched
from the data store. 
Following it, the third operation fetches the chosen pool for the returned  \gls{vip} \footnote{The current implementation of this
application always chooses the first existent pool. This behaviour
exists because the original developers anticipated a more robust Load Balancer application
where different pools can be associated with a \gls{vip} to enhance
the balancing algorithm with feedback of the application servers or
possibly the network state.}.

Afterwards it updates the fetched  pool with the newly modified
\texttt{current-member}. The forth and final operation retrieves
the address information for the selected information. 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
Get VIP id for the destination IP & R & 104 & 8\\
Get VIP Info (pool information) & R & 29 & 509\\
Get the chosen pool & R & 30 & 369\\
Conditional replace pool after round-robin & W & 772 & 1\\
Read the chosen Member & R & 32 & 221 \\
\end{tabular}\caption[Workload lbw-0-ip-to-vip( IP packet to a VIP)
operations]{Workload lbw-0-ip-to-vip( IP packet to a VIP) operations
  and sizes (in bytes).}
\label{table:lbw-0-ip-to-vip}
\end{table}



\subsection{ARP Request}
This workload  results  from processing an ARP Request addressed at a
\gls{vip} address. The data store operations, summarized in Table
\ref{table:lbw-0-arp-request}, shows that two reads are
required. First, as previously seen,  it queries the data
store to check if the packet destination address is a \gls{vip} (1 read
needed). As it is, the controller then retrieves the \texttt{MAC} address for that
\gls{vip} server (so, another read is needed to obtain the \gls{vip} entity).

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Get VIP id for the destination IP  & R & 104 & 8\\
Get VIP info (proxy MAC address) & R & 29 & 509 \\\bottomrule
\end{tabular}\caption[Workload lbw-0-arp-request( Arp Request to a
VIP) operations]{Workload lbw-0-arp-request( Arp Request to a VIP)
 operations and sizes (in bytes).}
\label{table:lbw-0-arp-request}
\end{table}

\subsection{Optimizations}
\begin{table}[ht]
\small
\begin{tabular}{llccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & lbw-0 & lbw-1  & lbw-2 & lbw-3 & lbw-4 \\ \toprule 
%& &   \multicolumn{5}{c}{(Request, Reply)} \\midrule 
Get VIP id of destination IP  & R & (104,8) &\multirow{2}{*}{(104,509)} &  \multirow{2}{*}{(104,513)} &\multirow{2}{*}{\textbf{(62,324)}} & \multirow{2}{*}{-}    \\\cmidrule{1-2} 
Get VIP info (pool)   & R &  (29,509) & & & &   \\ \midrule 
Get the choosen pool  & R & (30,369)  &  - & (30,373) & -   & \multirow{3}{*}[-2mm]{\textbf{(11,4)}}  \\  \cmidrule{1-2} 
Replace pool after round-robin  & W & (772,1) & -
&\textbf{(403, 1)} &  - \\ \cmidrule{1-2}  
  Read the chosen Member &  R & (32,221) & - & (32,225) & \textbf{(44,4)} & \\\bottomrule  
\end{tabular}\caption[Load Balancer IP to VIP workload operations across
diferent implementations.]{Load Balancer  lbw-\textit{X}-ip-to-vip workload
  operations and respective sizes (in bytes) across diferent
  implementations. Bolded sizes represent significant differences
  across implementations. Sizes marked with \texttt{-} are equal to
  the previous. } 
\end{table}

\begin{figure}[ht]
% \CenterFloatBoxes
%\TopFloatBoxes  
% \BottomFloatBoxes
\begin{floatrow}
\ffigbox{%
  \includegraphics[scale=0.4]{../data/reportGenerator/lbw-0-ip-to-viplbw-1-ip-to-viplbw-2-ip-to-viplbw-3-ip-to-viplbw-4-ip-to-viptxLatCmp.pdf}
}{\caption{Cenas}%
}
\capbtabbox{%
\small
  \begin{tabular}{lll} 
    Prefix &  Data store & Section\\\toprule
    lbw-0 & Simple Key-Value  & \ref{sec:}  \\
    lbw-1 & Cross References  & \ref{sec:} \\
    lbw-2 & Versioned Values & \ref{sec:} \\
    lbw-3 & Column Store & \ref{sec:} \\
    lbw-4 & Micro Components & \ref{sec:} \\ \bottomrule
    & &  \\ 
    & &  \\ 
    & &  \\ 
    & &  \\ 
    & &  \\ 
  \end{tabular}
}{%
  \caption[Name guide to Load Balancer workloads]{Name guide to Load
    Balancer workloads.}\label{table:lb-versions}
}
\end{floatrow}
\end{figure}

Table \ref{table} shows us the all the modifications we have done to
the workload triggered by a packet addressed at a \gls{vip}
address. It is somewhat similar to previous workload tables (e.g,
\ref{tables}), but this time, we show the workload according to
different implementations of the application using a variety of data
store functionalities. This way we can verify the impact of each
functionality individually. For reference we prefix the workload name
with a different name: lbw-0, lbw-1, ..., lbw-4. The functionality
used with that prefix can be seen in table
\ref{table:lb-versions}. The prefix lbw-0 refers to the simple
key-value store implementation which has already been presented in
table \ref{table:lbw-0-ip-to-vip} that summarized  the workload 
operations. Message size is now grouped in tuples. When no difference is noticed between implementations we use the \texttt{-}
symbol instead of a tuple. Significant changes are emphasized in
bold. Also notice that some operations are merged together (first
two operations on workload lbw-1 and the last three in workload
lbw-4.

For the first improvement we eliminate the double step required to
obtain a \gls{vip} in the first two messages. This can be done with
the Cross Reference functionality by configuring the data store with
the information that each value of the \texttt{vip-ip-to-id} table
(consulted in the first workload operation) is actually a key for
the \gls{vip} table. Then the data store, can  --- in one operation
--- fetch the \gls{vip} for the provided \gls{ip} address. 

Next, in workload lbw-2 we cut an operation size in half, by replacing the conditional
replace (after round-robin) in the 4th line by a similar operation
that uses a version number, provided by the data store while reading
the \gls{vip} information (notice the increase by 4 bytes in the first
read caused by adding the version number of the \gls{vip} to the
reply). Now, by logical time stamping every data store value, we avoid
the use of replace operations that requiring sending both the old
value and the new value. This not only benefits size but also
simplifies client code (see section \ref{sec:heimdall:versions}). 


We improve from workload lbw-2 to lbw-3 by adding Column support to
the data store. Then we can replace the existing reads of \gls{vip}
(first two operations) and Members by reading  them partially instead
of reading them entirely. With \gls{vip} entities we do not actually
improve a lot because the local operations that follow the read in the
data store actually require a lot of the attributes of a \gls{vip} (it
will become clear why in the next section). But with Members we
improve by a factor of 56 in the return value because we only require
reading its \gls{ip} attribute. 

Final we get to the most significant improvement by using
Micro-Components to provide a data store method that performs the
round robin operations and returns the Member \gls{ip} in only one
step. This table actually does not shows the true beneficial of this
optimization. Before it, we actually fetched and updated a pool in two
separate steps with the conditional replace. This is a potentially
bottleneck under high-peak utilization of the balanced resources
because we suspect this optimistic concurrency control mechanism will
fail a lot when different controllers receive significant concurrent
traffic addressed at the same \gls{vip}.  In the next section it will become clear why it is beneficial
to have two separate methods to fetch a \gls{vip} and round-robin. \\

\todo{Fazer um teste simple a mostrar que falhava era bem mais
  cientifico do que suspect} 

\todo{Tenho que explicar a figura de novo? Já expliquei como é que
  funciona na secção do Device Manager. Copy paste?} 

In Fig. \ref{fig:lbw:comparision} we see the performance results of
the different workloads. We can see observe the same patterns of the
previous performance analysis we done with Learning Switch (see
section \ref{section} ). Again, the size reduction improvements seem
to have little to no effect from workloads  lbw1 to lbw-3. This is
more expected since the improvements actually have a smaller  impact
in the workload then in the case of the Learning Switch. We can also
see that as before, message reduction has the greatest impact. \textbf{From
workload lbw-0 to lbw-1 we see a minor improvement from 4.5kFlows/s
with 4 ms latency to 6.1kFlows/s to 6.7 ms. }. Better than that we see
that with the final workload (lbw-4) we improve to 12kFlows/s  under
5ms latency. An improvement of more than double from the original
workload, with only a 25\% increase in latency. 


\subsection{Cache}
Caching in the Load Balancer case has effects on the consistency as
opposed to the Learning Switch case. This is because the Load Balancer
data is expected to shared and manipulated by different controllers
(unless we avoid by design and configuration). Still we explore this
possibility and defend, advocating that it may not be harmfull to do
so. 

Considering the ip-to-a-vip workload and use case. If we use cache to
maintain \gls{vip} entities locally on a time based manner. We can
benefit a lot. In the worst case the local \gls{vip} information can
be stale. Then clients may try to reach a VIP that does not exists
anymore for some reason or may have changed \gls{ip}  address to
provide another service. This is something that already can happen in
the strongly consistent version. When we fetch a \gls{vip} from the
data store it can already be invalid.  byt the time we get it
locally. But with cache this probability is greater.

\todo{We can actually solve this by using the version number in the
  round robin mechanism. Ou então ser for mesmo uma VIP vai logo à
  base de dados fazer round robin. E na cache ficas com blooms filters
  para dizer aqueles que não são vips. Falta implementar. }

On the other hand caching a \gls{vip} can be very beneficial because
the Load Balancer has a minimal impact on the controller pipeline of
one read in the data store. Even when processing a normal packet, not related to a VIP address at
all, the Load Balancer still has to find out if this is the case. This
workload, which only requires one operation (see table (First line of tables))
\ref{table:lbw-0-ip-to-vip,table:lbw-0-arp-request} but with  a 0 byte
reply) sets the minimum amount of work imposed by
the Load Balancer to the controller pipeline. 
requires this read operation. It would be great to avoid this
behaviour because we do not want to limit the pipeline to nearly 15.5kFlows/s
under 3ms latency (which is the data store performance when reading an
\gls{vip} as seen in Figure \ref{fig:}). 

As a final note, caching is actually the reason why the fetch
operation of a \gls{vip} described in the previous section actually
does not bring a significant improvement. This is not accidental but
by choice. The reason is that \gls{vip} are actually read for two
different functions: packet to a vip and arp request to a vip. So we
believe it is simpler and more efficient to actually fetch to the
cache the union of attributes required by the two different
cases. This makes sense because the two events are not independent. An
\gls{arp} request addressed at a \gls{vip} is usually followed by an
\gls{ip} packet to the same \gls{vip}.  

The time the \gls{vip} is considered valid in cache is
configurable. To be efficiently used the user should consider both the
dynamic changes frequency done to a \gls{vip} and the time under which
they take effect, as well as the probability of successive arp and ip
packets to the same \gls{vip}

%Ideally we should also avoid the normal case of IP packets not
%addressed at a VIP. For this our cache  must understand what a empty
%value means FIXME. (use containsInCache . update to insert empty in
%cache. Then see if containsInCache AND get == null you can be certain
%the value is not a VIP), completely avoiding the going to the data store.




\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{./../data/reportGenerator//lbw-3-ip-to-notviptxLat.pdf}
\caption[Minimum impact of Load Balancer in the pipeline.]{Workload
  lbw-3-ip-to-notvip shows the minimal impact the Load Balancer
  applications has on the pipeline in our best implementation.}
\end{figure}


\section{Device Manager}
\label{sec:feasibility:dm}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Device Manager application tracks and stores host devices
information such as the switch-ports to where devices are
attached to\footnote{The original application is able to track devices as
  they move around a network, however our current implementation does
  not take this feature into consideration.}. This information ---
that is retrieved from the OpenFlow packets that the controller receives --- is crucial to
Floodlight’s Forwarding application. That is to say, that for  each new flow, the Device
Manager has the job of finding a switch-port for both the destination
and source address. Given this information, it is able to pass it to
the Forwarding application, that can later decide on the actions to
take (e.g., best route). Notice that this arrangement, excludes the
Learning Switch as the  forwarding application in action. 

Regarding the data store usage, Device Manager requires three
data store tables listed in table \ref{table:dm:indexes}.  The first
table, \texttt{devices} keeps track of known devices created by the
application. A second table named \texttt{macs}  indexes those same devices by their
\gls{mac} and \gls{vlan}  pair.  Finally, a third table named
\texttt{ips} maintains an index from an \gls{ip} address to one or
more devices.

\begin{table}
\small
\begin{tabular}{cccc}
Name & Key & Value & \\ \toprule
devices & device-id &  device \\\midrule
macs & (MAC,VLAN)  & device-id   \\\midrule
ips  & IP & device-id list \\\midrule
\end{tabular}
\caption[Device Manager key-value tables]{Device Manager key-value tables.}
\label{table:dm:indexes}
\end{table}

We will analyze and improve on  two distinct workloads for this application differing in
wether the application already knows the source device information (figure \ref{fig:dm:interaction:known})
or not ( \ref{fig:dm:interaction:unknown}). 

In the former case, the
application mainly reads information from the data store in order to
obtain location information. As for the latter case, the
application must create the device information and updates all the
existent tables. Therefore, this workload generates more traffic between
the controller and data store. 


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/dm-unknown}
                \caption{Packet from an unknown device.}
                \label{fig:dm:interaction:unknown}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Packet from a known device.}
                \label{fig:dm:interaction:known}
        \end{subfigure}
        \caption[Device Manager workload events]{Workloads for this application heavily depend on the state of the data store. Unknown devices trigger several operations to the creation of these, while known devices only require an update of their "last seen" timestamp. No matter the case, the source and destination devices are retrieved if they exist.}
        \label{fig:dm:interaction}
\end{figure}

\subsection{Known Devices}

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Read the source device key & R & 408 & 8\\
Read the source device & R & 26 & 1444\\
Update "last seen" timestamp & W & 2942 & 0\\
Read the destination device key & R & 408 & 8\\
Read the destination device & R & 26 & 1369 \\
\end{tabular}
\caption[Workload dm-0-known (Known Devices) operations]{Workload
  dm-0-known (Known Devices) operations and sizes (in bytes).}
\label{table:ops:dm-0-known}
\end{table}

When devices are known to the application, a \texttt{packet-in} request
triggers the operations seen in table \ref{table:ops:dm-0-known}. The
first two operations read source and destination device
information in order to make their switch-ports available to the
Forwarding process. Additionally, the second operation (a write), 
updates the ``last seen'' timestamp of the source device.

\subsection{Unknown Source}
\small
\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
1) Read the source device key & R & 408 & 0\\
2) Get and increment the device id counter & W & 21 & 4\\
3) Put new device in device table & W & 1395 & 1\\
4) Put new device in \texttt{(MAC,VLAN)} table & W & 416 & 0\\
5) Get devices with source IP & R & 386 & 0\\
6 ) Update devices with source IP & W & 517 & 0\\
7) Read the destination device key & R & 408 & 8\\
8) Read the destination device & R & 26 & 1378 \\\bottomrule
\end{tabular}
\caption[Workload dm-0-unknown( ARP from Unknown Source)
operations]{Workload dm-0-unknown( ARP from Unknown Source) operations
  and sizes (in bytes).}
\label{table:ops:dm-0-unknown}
\end{table}


This workload is triggered in the specific case in which  the source device
is unknown and the \gls{of} message carries an \gls{arp} reply 
packet. Seing that both these  conditions are true, the application
proceeds  with 8 data store operations, described in table
\ref{table:ops:dm-0-unknown}. Their intention is to create device
information and update the three tables described  in the beginning
of this section.  

The first operation reads the  source device key. Being
that it is not known, this operation fails (notice in the table, that
the reply has a size  of zero bytes). As a result the application
proceeds with the creation of a device. For this, the
following write (second operation) atomically retrieves
and increments a device unique \texttt{id} counter. Afterwards, the third and fourth  operation
update, with the newly created device, the device and MAC/VLAN
tables respectively. Likewise, the fifth and sixth operations update
the \gls{ip} index table. Given that this index links an \gls{ip} to
several devices we are forced to first collect the set of devices in
order to update it. This \emph{read-modify} operation can
fail in case of concurrent updates. Under that case, both operations
would be repeated until they succeed. At this point, the Device Manager
is done with the creation of the device and can, finally, move to the
last two operations to fetch the destination device information. 

\subsection{Optimizations}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data/reportGenerator/dm-0-unknowndm-1-unknowndm-2-unknowndm-3-unknowndm-4-unknowntxLatCmp.pdf}
                \caption{}
                \label{fig:}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data/reportGenerator/dm-0-knowndm-1-knowndm-2-knowndm-3-knowndm-4-knowntxLatCmp.pdf}
                \caption{}
                \label{}
        \end{subfigure}
        \caption[Device Manager performance analysis]{}
        \label{fig:dm:performance}
\end{figure}

\begin{table}
\small
\begin{tabular}{lll} 
    Prefix &  Data store & Section\\\toprule
    dm-0 & Simple Key-Value  & \ref{sec:heimdall:datastore:kv}  \\
    dm-1 & Cross References  & \ref{sec:heimdall:datastore:cr} \\
    dm-2 & Versioned Values & \ref{sec:heimdall:datastore:vr} \\
    dm-3 & Column Store & \ref{sec:heimdall:datastore:cr} \\
    dm-4 & Micro Components & \ref{sec:heimdall:datastore:mc} \\ 
  \end{tabular}
  \caption[Name guide to Device Manager workloads]{Name guide to
    Device Manager workloads.}
  \label{table:names:dm}
\end{table}

\begin{table}[ht]
\small
\centering
\begin{threeparttable}
\begin{tabular}{ll ccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & dmw-0 & dmw-1  & dmw-2 & dmw-3 & dmw-4 \\ \toprule 
Get source key & R &(408, 8) & \multirow{2}{*}{(408,1274)} &
\multirow{2}{*}{(408,1278)} & \multirow{2}{*}{(486,1261)} &
\multirow{2}{*}{(28,1414)} \tnote{a} \\ \cmidrule{1-2}
Get source device & R & (26,1444) & & & & \\ \midrule
Update timestamp & W & (2942,0) & (2602,0) & \textbf{(1316,1)} & (667,1) & 
(36,0) \\ \midrule
Get destination key & R & (408,8) & \multirow{2}{*}[-1mm]{(408,1199)} &
\multirow{2}{*}[-1mm]{(408,1203)} & \multirow{2}{*}[-1mm]{(416,474)} &
\multirow{2}{*}[-1mm]{N/A} \\ \cmidrule{1-2}
Get destination device & R & (26,1369) &  &
 & & \\\bottomrule
\end{tabular}
\caption[Workload dm-0-known( Known Devices) operations]{Workload
  dm-0-known( Known Devices) operations and sizes (in bytes).}
\begin{tablenotes}
\item [a)] This operation also fetches the destination device.
\end{tablenotes}
\end{threeparttable}
\end{table}

%TODO - do not use put new device in MAC,VLAN table. This is
%confusing. 

\begin{table}[ht]
\small
\centering 
\begin{threeparttable}
\begin{tabular}{ll ccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & dmw-0 & dmw-1  & dmw-2 & dmw-3 & dmw-4 \\ \toprule 
Read source key & R & (408,0) & - & - & (486,0) & (28,201)\tnote{a}\\
Increment counter & W & (21,4) & -  & - & - & \multirow{5}{*}{(476,8)} \\
Update device table & W & (1395,1) & (1225,1)\tnote{b}  & - &
(1183,1) & \\
Update MAC  table & W & (416,0) & - & - & -
& \\
Get from IP index & R & (386,0) & - & - & - & \\
Update IP index  & W & (517,0) & - & - & - & \\
Get destination key & R & (408,8) &
\multirow{2}{*}{(408,1208)}\tnote{b} & \multirow{2}{*}{(408,1212)} &
\multirow{2}{*}{(416,474)} & \multirow{2}{*}{N/A}  \\ 
Get destination device & R & (26,1378)  &  & & \\\bottomrule
\end{tabular}
\caption[Workload dm-0-unknown( ARP from Unknown Source)
operations]{Workload dm-0-unknown( ARP from Unknown Source) operations
  and sizes (in bytes).}
\begin{tablenotes}
\item [a)] This operation also fetches the destination device.
\item [b)] Differences in sizes caused by a SERIALIZATION improvement 
\end{tablenotes}
\end{threeparttable}
\end{table}


First we  replace the two step operation required to fetch a device by
using Cross References tables (dm-1). Just as we have done in
analogous Load Balancer workload.  Next, we substitute the replace
operation in dm-1 by a version based replace. 

Columns (dm-3) we put the device. We  nearly do not improve  while
fetching the source device since the local logic requires reading
almost all the device attributes, but at least we do not incur in any
overhead associated with the column serialization logic. On the other
end the update timestamp operation has half the request size. We do
not improve more because the update timestamp updates a specific
element inside an array, so we actually have to replace the all array
since our update is limited. As for the destination device we can
reduce it nearly by a factor of three. 

Finally with micro components we can improve a lot. First we can
merge the source and destination device in only one operation (which
could also be done with transactions also). Following we can also
create a micro component to update the timestamp in the data
store. For this we only need to send the device key and new
timestamp which greatly improves the operation size.  

Also with workload dm-4-\ref{table:} we can improve by replacing
operations 2 to 6 by only operation that crates a new device. Again,
as in the known-devices workload we can fetch the source and
destination device information simultaneously. 

We can merge all operations (reading devices and creating them)
because we need to consult a local service (the topology manager) in
the controller before creating devices.  This is because we actually
have not shown other workloads for the Device Manager application
(e.g., when updating new \gls{ip} addresses). 



\subsection{Cache}
With cache we fetch known devices to the cache. Then in the known
devices workload case when we update the timestamp with the local version
number, the operation will only succeed if we have the most recent
device version.  \textbf{We expect this replace operation to succeed a lot of
time since we do not anticipate scenarios where different controllers
manipulate the same devices. But they can happen}. In practice this is
no different as the previous situation where the replace operation
could also fail (without cache). \textbf{If the devices are connected to different openflow
islands simultaneously than this is a bad idea since we will actually
have to perform one more request that the normal workload
pattern. (try to updated - fails, retrieve new , update) . Off course
this could be mitigated by having the update attempt to return the
currently present value timestamp}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../PEI"
%%% End: 

