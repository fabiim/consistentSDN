%\ref{sec:heimdall:datastore:functionalities}
% \ref{sec:heimdall:datastore:bft-smart}
%\ref{sec:heimdall:datastore:functionalities}
%\ref{section:background:of}
%!TEX root = ../PEI.tex
\label{sec:feasibility:apps}
\glsresetall

%\todo{Online appendix}

To evaluate the feasibility of our distributed controller design we implemented a prototype of the previously described controller architecture by integrating applications from the Floodlight controller\footnote{\url{http://www.projectfloodlight.org/floodlight/}} with a data store built over a state-of-the-art \gls{smr} library, BFT-SMaRt~\cite{smart-tr}.
We considered three SDN applications provided with Floodlight: \emph{Learning Switch}  (a common layer 2 switch), \emph{Load   Balancer} (a round-robin load balancer) and \emph{Device Manager} (an application that tracks devices as they move around a network).
In this chapter, we describe how they work and how we modify them.  Then we expose them as particular workloads that are later used in our performance analysis to the data store. 

%The applications were slightly modified but  we made an effort to avoid behavioral changes to the applications. The main change was shifting state from the controller's (volatile) memory to the data store efficiently (i.e., always trying to minimize communication). To the best of our knowledge the exposed services of the applications we changed are virtually indistinguishable from their predecessors.
%The objective of the experiments covered in this chapter  is to analyze the workloads generated by these applications to thereafter measure the performance of the data store when subject to such realistic demand caused by real applications.


A workload is a trace (or log) of data store requests  defined as a product of a data plane event, controller application and system global state (controller and data store).
Fig. \ref{fig:feasibility:workloads} exemplifies this definition: to the left we see a data plane event, triggered from the switch to the controller that, in turn, exchanges a specific sequence of messages with the data store (at the right) required to answer the event.

This chapter shows different workloads for the three applications modified as well as the iterative improvement process done with each of the data store functionalities described in section~\ref{sec:heimdall:datastore:functionalities}. 
Those functionalities are a consequence of the study of the workloads existent in the initial integration of the applications (in their original state), to the data store.
Documenting this process emphasizes the methodology  used, which can (arguably) be helpful to understand patterns that are far from optimal when adapting existent centralized applications to a key value store. 

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.7]{pic/feasibility/workload-generic.pdf}   
  \caption[Workload definition]{Each data plane events triggers a variable number of operations in the data store. The trace of those operations and their characteristics is a workload.}
  \label{fig:feasibility:workloads}
\end{figure}

More importantly, we use workloads to perform our feasibility study, composed of three phases. 
First, we emulated a network environment in Mininet~\cite{Handigol:2012tg} --- a network emulation platform that enables a virtual network, (running a real kernel, switch and application code on a single machine) ---  that consisted of a single switch and at least a pair of host devices.
Then \gls{icmp} requests (pings) were generated between the latter. 
The goal was to create \gls{of}  traffic (\texttt{packet-in} messages) from the ingress switch to the controller.
Then, for each \gls{of} request, the controller performs a variable, application-dependent number of read and write operations, of different sizes, in the data store (i.e., the \textit{workload}). 
In the controller (the data store client),  each data store interaction is recorded entirely (i.e., request and reply size, type of operation, etc.,) and associated with the data plane event that has caused it. 

Second, the collected workload traces were used to measure the performance of our distributed data store.
For this, we set up an environment in our cluster composed of four machines, three for the distributed data store (to tolerate one crash ($f=1$) three replicas are needed, as explained in Section~\ref{sec:relatedWork:consistentDataStore}), and one to simulate the data store client (the controller). 
This client will concurrently replay a simulation of the recorded workload with equal payloads (i.e., equal message type and size) as well as an additional 4 byte field representing the expected reply size of the data store response. 
Then a simple data store server, meant to record the throughput, will reply to the client messages. 
We do not use our data store prototype  because our  goal is to evaluate the performance of the BFT-SMaRt middleware since it is the bottleneck of our design performance. 
The simulation is repeated for a variable number of concurrent data store clients (representing different threads in one controller and/or different controllers).
From it we obtain throughput and latency benchmarks for the data store under different realistic loads (equivalent to data plane events).  

In the third phase, we analyze the results of the previous phases.
Then we improve by using the data store functionalities referred in Section~\ref{sec:heimdall:datastore:functionalities}. 
Afterwards we restart phase 1. 

Each workload (with all its composing operations)  was run 50 thousand times, measuring both latency and throughput. 
We measure averages, minimums, maximums and standard deviations in the 90, 95 and 99th percentile. 
Unless stated otherwise the values shown in this chapter are in the 90th percentile. 

Appendix A and B (available online~\cite{support})  all the benchmark information (in graphical and raw format) and the traces (i.e., data plane events and respective workloads) for each workload shown in this chapter.  We also made available the scripts that automate the data plane events in Mininet used in our experiments as well as instructions and the original codebase that can be used to reproduce  all the exposed work. 

Each machine in the performance benchmarks had two quad-core 2.27 GHz Intel Xeon E5520 and 32 GB of RAM memory, and they were interconnected with gigabit Ethernet. 
There were running  Ubuntu 12.04.2 LTS with  Java(TM) SE Runtime Environment (build 1.7.0\_07-b10) 64 bits.

% \begin{table}[ht]
%   \small
%   \centering
%   \begin{tabular}{lll}
%     Application & Version \\ \toprule 
%     Mininet |  Mininet 2.0 ()
%   \end{tabular}
%   \caption{Applications versions and pointers}
% \end{table}
For the applications, we  used Mininet 2.0 \footnote{Available at \url{http://mininet.org} (mininet-2.0.0-113012-amd64-ovf). We had an issue with this version and corrected it by following online instructions available at \url{http://goo.gl/DQ7FQF}.},  a Floodlight fork \footnote{\url{http://goo.gl/RbBXag} commit 9b361fbb3f84629b98d99adc108cddffc606521f} and  BFT-SMaRt\footnote{\url{http://code.google.com/p/bft-smart} revision 334}.  


\section{Learning Switch} 
\label{sec:feasibility:ls}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-broadcast}
                \caption{Broadcast packet.}
                \label{fig:ls:interaction:broadcast}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Unicast packet.}
                \label{fig:ls:interaction:unicast}
        \end{subfigure}
        \caption[Learning Switch workloads]{Operations in the data store vary based on wether the \gls{of} \texttt{packet-in} is triggered by a broadcast or unicast packet.}
        \label{fig:ls:interaction}
\end{figure}

The Learning Switch application emulates the hardware layer 2 switch forwarding process based on a switch table to associate  \gls{mac} addresses to the switch ports where they were last seen. 
Similarly, in the application, for each switch a different \gls{mac}-to-switch-port table is maintained in the data store. 

Each table is populated using the source address information (i.e., \gls{mac} and switch port)  present in every \gls{of}  \texttt{packet-in} request for the purpose of maintaining the location of devices. 
After learning this location, the controller can install rules in the switches to forward packets from a source to  a destination. 
Until then, the controller must instruct the switch to \emph{flood} the packet to every port, with the exception of the ingress port (where the packet came in from).

Despite being a single-reader and single writer application (each switch table is only accessed by the controller managing the switch), we include it here for two reasons: $(i)$ it benefits from the fault-tolerant property of our distribution process and $(ii)$ it is commonly used as the single-controller benchmark application in the literature~\cite{Tootoonchian:2012uia,Erickson:2013er}. 

Fig.~\ref{fig:ls:interaction}  shows the detailed interaction between the switch, controller (Learning Switch) and data store for two possible cases. 
First, the case for broadcast packets that require one write operation to store the switch-port  of the source address (Fig.~\ref{fig:ls:interaction:broadcast}). 
Second,  the case for unicast packets, that not only stores the source information, but also reads the (possibly) known switch port  for the destination address (Fig.~\ref{fig:ls:interaction:unicast}). 

\todo{tens que falar na tabelas locais e o catano} 
%In its original state this application maintained an hash table associating a switch to another hash table
%relating  \gls{mac} and \gls{vlan} to switch ports. Both this hash tables were thread-safe (i.e., supported concurrent manipulation safely). This fits naturally in our key-value data store client implementation. The smart reader will wonder why does the original application is single-reader, single
%writer. This happens since the internal state is actually exposed through the northbound \gls{api} existent.  

It is critical, both for the centralized and fault-tolerant version, that each switch table is limited due to resource exhaustion (each table can potentially keep an entry for each host present in the network!).
For this reason the application limits a table to a fixed number of hosts (1K by default).
When this limit is reached the least recently used entries are replaced for new ones.  
This eviction policy favors inactive devices over actives ones.
Each access to the table (either a read or a write, even for existent keys) promotes the key to the top of a list making it the most recently used.
After the table is full, newly added entries replace the bottom entry of the same list (the least referenced). Our data store allows the creation of tables with identical behavior. 

% way better: 
%The LRU discards the least recently used items first. For this,
%whenever an entry is accessed, it moves to the top of the
%list. Whenever and entry is added to the table, but the  capacity is
%in the limit, the last entry in the access list is removed, to give room to the new one.
The \gls{lru} tables are not the only way to control the table entries. Learning Switch also applies timeouts (hard and soft --- see section~\ref{section:background:of})  to the flows installed in the data plane. When they expire, a switch triggers an \gls{of} \texttt{FLOW\_REMOVED} message (containing a source and a destination address) to the control plane that, in turn, deletes the associated entry from the data store and instructs the switch to remove the reverse flow rule entry (from the destination to the source) from its table. Then, this process is repeated one more time. 

\subsection{Broadcast Packet}
This workload is defined by  the operations performed in the data store when processing broadcast packets in an \acrfull{of} \texttt{packet-in} request (Fig.~\ref{fig:ls:interaction:broadcast}). Table~\ref{table:lsw0:broadcast} shows that for the purpose of associating the source address of the packet to the ingress switch-port where it was received, the Learning Switch application performs one write operation with a request size of 113 bytes and reply size of 1 byte (reporting success). 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
 Associate source address to ingress port & W & 113 & 1 \\ \bottomrule
\end{tabular}
\caption[Workload lsw-0-broadcast operations]{Workload lsw-0-broadcast operations and sizes (in bytes).}
\label{table:lsw0:broadcast}
\end{table}


Notice that in order to tag the \emph{source-to-port} entry as the most recently used (in the \gls{lru} table) the Load Balancer has to perform this write regardless of the entry being already known or not. 
\subsection{Unicast Packet}
This workload adds an operation to the previous one, since for every unicast packet we must also fetch the known switch port location of the destination address. Table \ref{table:lsw0:unicast} shows that this second operation has a weight of 36 bytes for the request payload (sent to the data store) and a 77 byte response size containing the known switch port.  

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply  \\ \toprule 
Associate source address to ingress port & W & 113 & 1\\
Read egress port for destination address & R & 36 & 77 \\ \bottomrule
\end{tabular}
\caption[Workload lsw-0-unicast operations]{Workload lsw-0-unicast operations and sizes (in bytes).}
\label{table:lsw0:unicast}
\end{table}

\subsection{Optimizations}
\label{sec:optimizations-2}
The Learning Switch operations are simple, so there is not much we can do to improve. 
Still, there is an overhad in the messages exchanged considering their content: a MAC address (6 byte standard); and a switch port identifier.  
This  is justified by the fix overhead of the Java Object Serialization Stream Protocol that we use to transform the object values into byte arrays (as required by our data store prototype). 
By doing it  manually  we lower the total size of the messages in the unicast workload by 72\% (see Table~\ref{table:lsw1:unicast}). The same goes for the broadcast workload (first line of the same table). 

Fig. \ref{fig:lsw:comparison} shows the results of the performance analysis made to the data store using the four workloads (the methodology is described in the beginning of this chapter). 
The reported metric for the average throughput  is Flows per second where each flow  is equivalent to the executing of all the workload steps. Similarly, the measured latency is taken per flow. 
The resulting values follow an exponential growth as we increase the load on the system by adding more clients. 

Surprisingly, the difference in performance between the original versions (with workload names prefixed by lsw-0) and the optimized size versions (prefixed by lsw-1) is unnoticeable. In some points lsw-1 is worst than lsw-0 due to the \textbf{statistical variance (?)}. 
It must be that the network packet exchanged by clients and the data store (or between the data store replicas) is  not affected (in size) by this change. 
Indeed, we will soon verify that size optimizations are bearably unnoticeable in all our examples, except when differing in order of magnitude. 

The results also show a significant difference between unicast and broadcast workloads caused by the number of messages. 
For the broadcast workload (1 message) the data store can support up to 20kFlows/s under a 3 ms, and 
12kFlows/s with the same 3ms latency  for the unicast workload (2 messages). 
%The natural conclusion we can take, is to think that if we merge the two messages that compose the broadcast workload into one (by using Micro Components described in section \ref{sec:heimdall:datastore:mc}) we should obtain performance results equivalent to the broadcast workload. 
%This is true, but with the introduction of caching to the Learning Switch application we will show why this is not necessary. 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Associate source address to ingress port & W & 29 & 1\\
Read egress port for destination address & R & 27 & 6 \\ \bottomrule
\end{tabular}
\caption[Workload lsw-1-unicast operations]{Workload lsw-1-unicast operations and sizes (in bytes).}
\label{table:lsw1:unicast}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{../data/reportGenerator/lsw-0-broadcastlsw-0-unicastlsw-1-broadcastlsw-1-unicasttxLatCmp.pdf}
\caption[Learning Switch workloads performance comparison]{Learning
  Switch workloads performance comparison (90th percentile). }
\label{fig:lsw:comparison}
\end{figure}

\subsection{Cache}
\label{sec:ls:cache}
%Before we delve into caching the Learning Switch tables in the controller we must further understand this application in order to understand how we have impact in its centralized and original behaviour. 
Given that Learning Switch is a single reader, single writer application, we can introduce caching mechanisms without impairing the consistency semantics. 
On the extreme, the Learning Switch could contain an exact copy of everything that is present in the data store at every time.  
With cache we can potentially avoid the data store while processing a requests, thus avoiding the two operations in the common unicast packet workload. 

First, we can avoid re-writing the source address to source port association when we already now it. 
In the original Learning Switch this re-write is not expensive (since it is local) and tags the source device  entry as the most recently one in the \gls{lru} table. 
But with the data store this write operation becomes expensive (3 ms under acceptable load).
However if we avoid updating the data store, then the active host may be forgotten sooner than before. Still we project that this will not be an issue since a host, being active, will benefit in latency before the data store removes him. %Notice that while avoiding this write, in our cache implementation,  we must verify that  it is not new and actually a different association caused by a device moving ports.

Second, we can also avoid the read operation that queries the egress port of the currently processed packet if that entry is  available in cache. 
With this improvement, we no longer have to read values from the database as long as they are available in cache and we still get consistency because when we update a value we also update the cache. 

\todo{code: add reading from the data store if not in cache. }

%The reader may think of an exception where we find stale data: when a host moves from a switch to another, the tables from the first switch will have  incorrect data and devices will be unreachable from that switch from some time. But this also happens with the centralized version also. This is why rules installed in the switches must have a idle and hard timeout set. When one of the timeouts expire the switch triggers an \texttt{FLOW\_REMOVED} messae to the controller, that in turns deletes the respective information in the data store.  This kind of problems reside on the data plane consistency side. Not on the control plane. 

%Cache can only improve on the overall analysis of the overall system (controller and data store) since the cache logic actually resides on the controller and not the data store.

\todo{Finish up with reference or explanation of why we did not do it}
%We don't improve on the workload.  
%We don't actually improve on the micro-benchmarks tested measures
%shown throughout this chapters. We do not improve simply because with
%cache we do not avoid or improve (by size reduction) any of the data
%store interactions present in table \ref{table:work:lsw1-1} (that
%shows the latest learning switch workload).  With cache we will only
%improve on the long run, since we can now avoid the two type of
%requests present in that table.

\section{Load Balancer}
\label{sec:feasibility:lb}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{In Device Manager and Load Section say that the results can vary.}
The Load Balancer application employs a round-robin\footnote{This algorithm distributes each request to a different server.} algorithm to distribute the requests addressed to a \gls{vip}.

Fig.~\ref{fig:lb-model} shows the entities relevant to understand this application. 
The \gls{vip} entity represents a virtual endpoint with a specified \gls{ip}, port and protocol (ICMP, TCP or UDP) address. 
Each \gls{vip} can be associated with one or more \emph{Pool}. Given that the distribution algorithm is round-robin, each \emph{Pool} has a current assigned server (\texttt{current-member} attribute). Finally, the third entity --- \emph{Member} --- represents a real server. 
Table~\ref{table:lb:indexes} shows the different tables required by  Load Balancer.
The first three, track entities by their key attributes. An additional table (\texttt{vip-ip-to-id})  links  \glsplural{ip} to \glsplural{vip}. 

\begin{figure}[ht]
\TopFloatBoxes
\begin{floatrow}
\ffigbox{
\includegraphics[scale=0.6]{./pic/feasibility/lb-model.pdf}
}{\caption[Load Balancer domain model]{Simplified Load Balancer entity model. Only the attributes relevant to our discussion are shown.}
\label{fig:lb-model}}


\capbtabbox{
\small
\begin{tabular}{cccc}
  Name & Key & Value & \\ \toprule
vips  & vip-id  & vip   \\
pools & pool-id &  pool \\
members & member-id  & member    \\
vip-ip-to-id &  ip & vip-id   \\\midrule
\end{tabular}
}{\caption[Load Balancer key-value tables]{Load Balancer key-value tables.}\label{table:lb:indexes}}
\end{floatrow}
\end{figure}

%TODO explain ARP Requests. 
For every \gls{of} \texttt{packet-in} request, Load Balancer asserts if it is addressed at a \gls{vip}. If so, two different executions flows are possible. First, as seen in Fig.~\ref{fig:lb:interaction:arp2Vip}, when the event is caused by an \gls{arp} request\footnote{An \gls{arp} request is a protocol used to translate \gls{ip} addresses into \gls{mac} addressed that are essential for communication in Ethernet based networks}, the Load Balancer, must fetch the \texttt{proxy-mac} address attribute of the \gls{vip} to reply to the source host.  

Second, if the event is caused by \gls{ip} data packets the application must: $(i)$ fetch the \gls{vip} information; $(ii)$ choose and fetch a \emph{Pool}; $(iii)$ rotate the \texttt{current-member} attribute of the \emph{Pool}  (to round-robin); and $(iv)$  fetch the chosen  \emph{Member}  data. Fig.~\ref{fig:lb:interaction:ip2Vip} shows this process (round-robin aggregates steps $(ii)$ to $(iv)$).  

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-broadcast}
                \caption{ARP packet address at a VIP.}
                \label{fig:lb:interaction:arp2Vip}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-unicast}
                \caption{IP packets addressed at a VIP. }
                \label{fig:lb:interaction:ip2Vip}
        \end{subfigure}
        \caption[Load Balancer workloads]{Load Balancer workloads by events.}  
        \label{fig:lb:interaction}
\end{figure}



\subsection{ARP Request}
Table~\ref{table:lbw-0-arp-request}  shows the operations that result from an \gls{of} \texttt{packet-in} caused by an \gls{arp} request querying the  \gls{vip} \gls{mac} address. 
In the first operation, Load Balancer attempts to retrieve the \texttt{vip-id} for the destination \gls{ip}. If he succeeds (reply is different than 0) then the retrieved \texttt{vip-id} is used to obtain the related \gls{vip} entity (entirely). Notice that the return size is 84 times bigger than a standard \gls{mac} address size (6 bytes). 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Get \texttt{vip-id} for the destination IP  & R & 104 & 8\\
Get VIP  & R & 29 & 509 \\\bottomrule
\end{tabular}\caption[Workload lbw-0-arp-request operations]{Workload lbw-0-arp-request operations and sizes (in bytes).}
\label{table:lbw-0-arp-request}
\end{table}


\subsection{Packets to a VIP}
Table~\ref{table:lbw-0-ip-to-vip} shows the detailed operations triggered by \gls{ip}  packets addressed at a \gls{vip}. 
The first two operations fetch the \gls{vip} entity associated with the destination \gls{ip} address of the packet. 
From the \gls{vip} we obtain the \texttt{pool-id} used to retrieve the \emph{Pool} (third operation)\footnote{Given that the current implementation always chooses the same  pool we could optimize this behaviour. However this will enable future improvements of the Load Balancing algorithm in place.}. 
The next step is to perform the round-robin algorithm by updating the \texttt{current-member} attribute of the retrieved \emph{Pool}. 
This is done locally. 
Afterwards, the fourth operation attempts to replace the data store \emph{Pool} by the newly update one. 
If the \emph{Pool} has changed between the retrieve and replace operation this operation fails (reply equal to 0) and we must try again by fetching the \emph{Pool} one more time. In order to check if the versions have changed, the replace operation contains both the original and updated versions to be used by the data store. 
To succeed the original client version must be equal to the data store version when processing the request.
If successful (reply equal to 1) we can move on and read the chosen \emph{Member} (server) associated with the \texttt{member-id}  that has been determined by the round robin algorithm. 

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
Get \texttt{vip-id} for the destination IP & R & 104 & 8\\
Get VIP & R & 29 & 509\\
Get the chosen pool & R & 30 & 369\\
Conditional replace pool after round-robin & W & 772 & 1\\
Read the chosen Member & R & 32 & 221 \\ \bottomrule
\end{tabular}\caption[Workload lbw-0-ip-to-vip operations]{Workload lbw-0-ip-to-vip operations
  and sizes (in bytes).}
\label{table:lbw-0-ip-to-vip}
\end{table}


\subsection{Optimizations}
\begin{table}[ht]
\small
\begin{tabular}{llccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & lbw-0 & lbw-1  & lbw-2 & lbw-3 & lbw-4 \\ \toprule 
%& &   \multicolumn{5}{c}{(Request, Reply)} \\midrule 
Get VIP id of destination IP  & R & (104,8) &\multirow{2}{*}{(104,509)} &  \multirow{2}{*}{(104,513)} &\multirow{2}{*}{\textbf{(62,324)}} & \multirow{2}{*}{-}    \\\cmidrule{1-2} 
Get VIP info (pool)   & R &  (29,509) & & & &   \\ \midrule 
Get the choosen pool  & R & (30,369)  &  - & (30,373) & -   & \multirow{3}{*}[-2mm]{(11,4)}  \\  \cmidrule{1-2} 
Replace pool after round-robin  & W & (772,1) & -
&\textbf{(403, 1)} &  - \\ \cmidrule{1-2}  
  Read the chosen Member &  R & (32,221) & - & (32,225) & \textbf{(44,4)} & \\\bottomrule  
\end{tabular}\caption[Load Balancer IP to VIP workload operations across
diferent implementations.]{Load Balancer  lbw-\textit{X}-ip-to-vip workload
  operations and respective sizes (in bytes) across diferent
  implementations. Bolded sizes represent significant differences
  across implementations. Sizes marked with \texttt{-} are equal to
  the previous. }\label{table:lbw:optimizations}
\end{table}

\begin{figure}[ht]
% \CenterFloatBoxes
%\TopFloatBoxes  
% \BottomFloatBoxes
\begin{floatrow}
\ffigbox{%
  \includegraphics[scale=0.4]{../data/reportGenerator/lbw-0-ip-to-viplbw-1-ip-to-viplbw-2-ip-to-viplbw-3-ip-to-viplbw-4-ip-to-viptxLatCmp.pdf}
}{\caption[Load Balancer lbw-X-ip-to-vip comparison]{Load Balancer ip-to-vip workload performance comparison.}\label{fig:lbw-ip-to-vip:comparison}%
}
\capbtabbox{%
\small
  \begin{tabular}{lll} 
    Prefix &  Data store & Section\\\toprule
    lbw-0 & Simple Key-Value  & \ref{sec:}  \\
    lbw-1 & Cross References  & \ref{sec:} \\
    lbw-2 & Versioned Values & \ref{sec:} \\
    lbw-3 & Column Store & \ref{sec:} \\
    lbw-4 & Micro Components & \ref{sec:} \\ \bottomrule
    & &  \\ 
    & &  \\ 
    & &  \\ 
    & &  \\ 
    & &  \\ 
  \end{tabular}
}{%
  \caption[Name guide to Load Balancer workloads]{Name guide to Load
    Balancer workloads.}\label{table:lb-versions}
}
\end{floatrow}
\end{figure}

Table~\ref{table:lbw:optimizations} shows all the optimizations done to the 
workload triggered by a packet addressed at a \gls{vip}
address.
It is similar to previous workload description tables but this time, we show how the data store functionalities affect the workloads. 
To simplify our discussion we prefix each workload with a different name: lbw-0, lbw-1,..., lbw-4. 
For reference, Table~\ref{table:lb-versions} relates prefixes to data store functionalities. 
Prefix lbw-0 refers to the initial key-value store implementation already presented (in Table~\ref{table:lbw-0-ip-to-vip}). 

In the first improvement (lbw-1) we eliminate the double step required to obtain a \gls{vip} (first two operations). 
This can be done with the Cross Reference functionality by stating, when creating the \texttt{vip-ip-to-id} table (consulted in the first workload operation)  that their values are key values of the \texttt{vips} table. Then the data store, can  in one operation  fetch the \gls{vip} for the provided \gls{ip} address. 

Next, in workload lbw-2 we reduce an operation size in half, by ``upgrading'' the conditional replace (after round-robin) in the 4th line to a similar operation based on  versions number that are  provided by the data store while reading the \gls{vip} information (notice the increase by 4 bytes in the first read caused by adding the version number of the \gls{vip} to the reply).  

For the next improvement (lbw-3), we configure the \texttt{members} and \texttt{vips} tables to keep values as columns. 
Then we can replace the existing read of a \gls{vip} (first two operations) and \emph{Members} by partial reads. 
With \gls{vip} entities we do not improve since the local operations that follow the read in the
data store actually require most of the \gls{vip} attributes (it will become clear why in the next section). 
On the other hand,  with Members we reduce the return value of the last operation  by a factor of 56 in the return value because we only require reading its \gls{ip} attribute. 

Finally, we get to the most significant improvement by setting up a method in the data store equivalent to the local round robin operation that also returns the Member \gls{ip} in a single step.
This table does not shows one benefit of this optimization: 
In the previous operations, we  fetch and update a \emph{Pool} in two separate steps with the conditional replace that can fail in case of concurrent updates. 
This is a potentially bottleneck under peaks of traffic directed at the balanced resources causing different  controllers (or even the same) to update the same \emph{Pool}. 

\todo{Fazer um teste simple a mostrar que falhava era bem mais
  cientifico do que suspect} 
\todo{Tenho que explicar a figura de novo? Já expliquei como é que
  funciona na secção do Device Manager. Copy paste?} 


Fig.~\ref{fig:lbw-ip-to-vip:comparison} shows the performance results of our analysis to the different workloads. We can identify the same patterns of the analogous analysis done to Learning Switch (see Section~\ref{sec:optimizations-2}). 


Again, the size reduction improvements seem to have little to no effect from workloads lbw1 to lbw-3. 
However this is more expected since the improvements actually have a smaller impact in the workload then in the case of the Learning Switch. 

We can also see that, as before, message reduction has the greatest impact. 
From workload lbw-0 to lbw-1 we see a minor improvement from 4.5kFlows/s with 4 ms latency to 6.1kFlows/s with 6.7 ms .
And the improvement is even better with the final workload (lbw-4) where we obtained 12kFlows/s  under 5ms latency. 
An improvement of more than double from the original workload, with only a 25\% increase in latency. 

\subsection{Cache}
With cache  we can maintain \gls{vip} entities locally. Then in the worst case the local \gls{vip} information can be stale.
This means that  clients that may try to reach a VIP that does not exists anymore or may have changed its \gls{ip}  address to provide another service. 
But this is something that already can happen in the strongly consistent version since when we fetch a \gls{vip} from the data store it can already be invalid by the time it arrives at the client. 
With cache this probability could become greater,  but it is tunable  since with our implementation has a time-based interface.

With \glsplural{vip}  kept in cache the Load Balancer can avoid reading from the data store which will set a theoretical maximum of (\textbf{inserir resultados de um teste só com o write}). This is the performance result of a workload based only on  the round-robin micro-component operation (described in Table~\ref{table:lbw:optimizations}). Also, notice that we can use this component to evaluate the staleness of the \gls{vip} used. If the \gls{vip} had changed in the meanwhile the data store could abort the operation and we could restart from scratch (thus never performing round robin in a stale \gls{vip} cached entry). 

However, this is not the only benefit.  With cache we can also benefit the throughput and latency of other applications. 
The Load Balancer has a minimal impact on the controller pipeline caused by reading from the data store for every \texttt{packet-in} request.  
Even when processing a normal packet, not related to a \gls{vip} address at all, the Load Balancer still has to find out if this is the case. 
This workload, which requires one operation to fetch the \gls{ip} for the destination \gls{ip} which will not exist  (see Table~\ref{table:lbw:optimizations})  sets an hard limit on the controller pipeline even the network traffic is hardly using the \gls{vip} resources. 
We estimate that this limit is in the order of 15.5kFlows/s under 3ms latency under acceptable load to the data store (see Fig.~\ref{fig:lb:minimal}).
With caching we can avoid this behavior by keeping empty values (values which do not exist in the data store).

As a final note, caching is the reason why we do not improve when using columns  to fetch the \gls{vip} information. 
This is not accidental but by choice. 
The reason is that Load Balancer reads \glsplural{vip} in two distinct cases: \gls{ip} packet to a \gls{vip} and \gls{arp} request to a \gls{vip}. 
So we believe it is simpler and more efficient to actually fetch to the cache the union of attributes required by the two different
cases. 
This is beneficial because the two events are not independent (it is very likely that an \gls{arp} request for a \gls{vip} is followed by an \gls{ip} packet for a \gls{vip}). 
An \gls{arp} request addressed at a \gls{vip} is usually followed by an \gls{ip} packet to the same \gls{vip}.  

%Ideally we should also avoid the normal case of IP packets not
%addressed at a VIP. For this our cache  must understand what a empty
%value means FIXME. (use containsInCache . update to insert empty in
%cache. Then see if containsInCache AND get == null you can be certain
%the value is not a VIP), completely avoiding the going to the data store.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{./../data/reportGenerator//lbw-3-ip-to-notviptxLat.pdf}
\caption[Minimum impact of Load Balancer in the pipeline.]{Performance analysis of fetching a non existing \gls{vip}. }\label{fig:lb:minimal}
\end{figure}

\section{Device Manager}
\label{sec:feasibility:dm}
\glsresetall

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Device Manager application tracks and stores host device information such as the switch-ports attachment points (ports where devices are connected to). 
This information --- that is retrieved from the \gls{of} packets that the controller receives --- is crucial to Floodlight’s Forwarding application. 
For each new flow, Device Manager retrieves the known switch ports for the source and destination address that are later used by other applications. 
Notice that the use of  Device Manager excludes the Learning Switch as the  forwarding application in action (otherwise it would be redundant since both track attachment points).

Device Manager requires three data store tables listed in Table~\ref{table:dm:indexes}.
The first table (\texttt{devices}) keeps track of known devices created by the application.
The second (\texttt{macs}),  tracks the same devices by their \gls{mac} address and \gls{vlan} identifier pair.
Finally, a third table named \texttt{ips} links  \glsplural{ip} addresses to one or more devices.
\todo{Learn why the hell does on ip links to more than one device.}

\todo{In Device Manager and Load Section say that the results can vary.}

\begin{figure}[ht]
\TopFloatBoxes
\begin{floatrow}
\ffigbox{
\includegraphics[scale=0.6]{./pic/feasibility/device-model.pdf}
}{\caption[Device Manager class diagram]{Simplified Device Manager class diagram. Only the attributes relevant to our discussion are shown.}
\label{fig:device-model}}

\capbtabbox{
\small
\begin{tabular}{cccc}
Name & Key & Value & \\ \toprule
devices & device-id &  device \\
macs & (MAC,VLAN)  & device-id   \\
ips  & IP & device-ids* \\\midrule
\end{tabular}
}{\caption[Device Manager key-value tables]{Device Manager key-value tables.}
\label{table:dm:indexes}}
\end{floatrow}
\end{figure}

This application extracts  switch port, \gls{mac} and \gls{vlan} information for the source address from every \gls{of} \texttt{packet-in} request processed by the controller. 
Then it can update or create devices based on that information. 
A device (see Fig.,~\ref{fig:device-model}) is uniquely identified by its \texttt{device-id} or \gls{mac} and \gls{vlan} pair.
It is also composed by one or more entities (\emph{Entity} class in the figure). 
Each entity is a ``visible trace'' of a device activity. 
In our experiences Device manager always creates two entities for each device: one associated with the device \gls{ip} address that is created (or updated) on every \gls{arp} request and a generic one (with ip 0.0.0.0) that is created or updated with every \gls{ip} data packet seen from that device. The \emph{last-seen} timestamps of the appropriate  entity is updated on every packet seen. This information is later used to age out the inactive devices. 

We analyze and improve on two workloads from this application. 
The first workload (Fig.~\ref{fig:dm:interaction:unknown})  is caused by an \gls{arp} packet from an unknown device,  and the second (Fig.~\ref{fig:dm:interaction:known}) by an \gls{ip} packet from a well-known device. 
In the former case, the application must create the device information and update all the existent tables. As for the latter case, Device Manager updates the source device \texttt{last-seen} timestamp. 
In both cases the known attachment points of both source and destination devices  are fetched  to be provided  to the Forwarding application. 

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/dm-unknown}
                \caption{Packet from an unknown device.}
                \label{fig:dm:interaction:unknown}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/dm-known}
                \caption{Packet from a known device.}
                \label{fig:dm:interaction:known}
        \end{subfigure}
        \caption[Device Manager workload events]{Workloads for this application heavily depend on the state of the data store. Unknown devices trigger several operations to the creation of these, while known devices only require an update of their "last seen" timestamp. No matter the case, the source and destination devices are retrieved if they exist.}
        \label{fig:dm:interaction}
\end{figure}

\subsection{Known Devices}

\begin{table}[ht]
\small
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
Read the source device key & R & 408 & 8\\
Read the source device & R & 26 & 1444\\
Update "last seen" timestamp & W & 2942 & 0\\
Read the destination device key & R & 408 & 8\\
Read the destination device & R & 26 & 1369 \\bottomrule 
\end{tabular}
\caption[Workload dm-0-known (Known Devices) operations]{Workload
  dm-0-known (Known Devices) operations and sizes (in bytes).}
\label{table:ops:dm-0-known}
\end{table}

When devices are known to the application, a \texttt{packet-in} request
triggers the operations seen in table~\ref{table:ops:dm-0-known}. The
first two operations read the source device information. 
Then an update is required to update the ``last seen'' timestamp of the device generic \texttt{entity}. 
Notice that the size of this request message is nearly the double of a device (1444 bytes). 
This is due to the fact this is a standard replace  containing both the original device (fetch in step \#2) and the updated device. 
This operation will fail if other data store client  has changed the device. If so, the process is restarted from the beginning. 
Otherwise, the last two operations can fetch the  destination device. 

\subsection{Unknown Source}
\small
\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
Operation & Type & Request & Reply \\ \toprule 
1) Read the source device key & R & 408 & 0\\
2) Get and increment the device id counter & W & 21 & 4\\
3) Put new device in device table & W & 1395 & 1\\
4) Put new device in \texttt{(MAC,VLAN)} table & W & 416 & 0\\
5) Get devices with source IP & R & 386 & 0\\
6 ) Update devices with source IP & W & 517 & 0\\
7) Read the destination device key & R & 408 & 8\\
8) Read the destination device & R & 26 & 1378 \\\bottomrule
\end{tabular}
\caption[Workload dm-0-unknown operations]{Workload dm-0-unknown operations
  and sizes (in bytes).}
\label{table:ops:dm-0-unknown}
\end{table}

This workload is triggered in the specific case in which  the source device is unknown and the \gls{of} message carries an \gls{arp} reply 
packet. 
If so, as seen in Table~\ref{table:ops:dm-0-unknown},  8 data store operations are required in order to create a device.  
The first operation reads the  source device key.  
Being that it is not known (notice in the table, that the reply has a size  of zero bytes) the application proceeds with the creation of the device. 
For this, the following write (second operation) atomically retrieves and increments a device unique \texttt{id} counter. 
Afterwards, the third and fourth  operation update, with the newly created device, the \texttt{devices} and \texttt{macs} tables respectively. 
Then, since the \texttt{ips} table links an \gls{ip} to several devices we are forced to first collect a set of devices  (operation \#5) in order to update it (operation \#6).  
This \emph{read-modify} operation can fail in case of concurrent updates.
However, we suspect that this will not be an issue since it is unusual to have a device updated by different controllers.
If successful, the Device Manager is done with the creation of the device and can, finally, move to the last two operations that fetch the destination device information. 


\note{falar com prof Ramos sobre a cena dos ips.  }
\subsection{Optimizations}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data/reportGenerator/dm-0-unknowndm-1-unknowndm-2-unknowndm-3-unknowndm-4-unknowntxLatCmp.pdf}
                \caption{Unknown device.}
                \label{fig:dm:comparison:unknown}

        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{../data/reportGenerator/dm-0-knowndm-1-knowndm-2-knowndm-3-knowndm-4-knowntxLatCmp.pdf}
                \caption{Known device.}
                \label{fig:dm:comparison:known}
        \end{subfigure}
        \caption[Device Manager performance comparison]{Device Manager performance comparison}
        \label{fig:dm:performance}
\end{figure}


\begin{table}[ht]
\small
\centering
\begin{threeparttable}
\begin{tabular}{ll ccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & dm-0 & dm-1  & dm-2 & dm-3 & dm-4 \\ \toprule 
Get source key & R &(408, 8) & \multirow{2}{*}{(408,1274)} &
\multirow{2}{*}{(408,1278)} & \multirow{2}{*}{(486,1261)} &
\multirow{2}{*}{(28,1414)} \tnote{a} \\ \cmidrule{1-2}
Get source device & R & (26,1444) & & & & \\ \midrule
Update timestamp & W & (2942,0) & (2602,0) & \textbf{(1316,1)} & (667,1) & 
(36,0) \\ \midrule
Get destination key & R & (408,8) & \multirow{2}{*}[-1mm]{(408,1199)} &
\multirow{2}{*}[-1mm]{(408,1203)} & \multirow{2}{*}[-1mm]{(416,474)} &
\multirow{2}{*}[-1mm]{N/A} \\ \cmidrule{1-2}
Get destination device & R & (26,1369) &  &
 & & \\\bottomrule
\end{tabular}
\caption[Workload dm-0-known operations]{Workload
  dm-0-known  operations and sizes (in bytes).}\label{table:dm-known-optimizations}
\begin{tablenotes}
\item [a)] This operation also fetches the destination device.
\end{tablenotes}
\end{threeparttable}
\end{table}



\begin{table}[ht]
\small
\centering 
\begin{threeparttable}
\begin{tabular}{ll ccccc}
 Operation & Type &  \multicolumn{5}{c}{ (Request, Reply) } \\  \midrule
&  & dmw-0 & dmw-1  & dmw-2 & dmw-3 & dmw-4 \\ \toprule 
Read source key & R & (408,0) & - & - & (486,0) & (28,201)\tnote{a}\\
Increment counter & W & (21,4) & -  & - & - & \multirow{5}{*}{(476,8)} \\
Update device table & W & (1395,1) & (1225,1)\tnote{b}  & - &
(1183,1) & \\
Update MAC  table & W & (416,0) & - & - & -
& \\
Get from IP index & R & (386,0) & - & - & - & \\
Update IP index  & W & (517,0) & - & - & - & \\
Get destination key & R & (408,8) &
\multirow{2}{*}{(408,1208)}\tnote{b} & \multirow{2}{*}{(408,1212)} &
\multirow{2}{*}{(416,474)} & \multirow{2}{*}{N/A}  \\ 
Get destination device & R & (26,1378)  &  & & \\\bottomrule
\end{tabular}
\caption[Workload dm-0-unknown operations]{Workload dm-0-unknown operations
  and sizes (in bytes).}\label{table:dm-unknown-optimizations}
\begin{tablenotes}
\item [a)] This operation also fetches the destination device.
\item [b)] Differences in sizes caused by a SERIALIZATION improvement 
\end{tablenotes}
\end{threeparttable}
\end{table}


\begin{table}
\small
\begin{tabular}{lll} 
    Prefix &  Data store & Section\\\toprule
    dm-0 & Simple Key-Value  & \ref{sec:heimdall:datastore:kv}  \\
    dm-1 & Cross References  & \ref{sec:heimdall:datastore:cr} \\
    dm-2 & Versioned Values & \ref{sec:heimdall:datastore:vr} \\
    dm-3 & Column Store & \ref{sec:heimdall:datastore:cr} \\
    dm-4 & Micro Components & \ref{sec:heimdall:datastore:mc} \\ 
  \end{tabular}
  \caption[Name guide to Device Manager workloads]{Name guide to
    Device Manager workloads.}
  \label{table:names:dm}
\end{table}


Table~\ref{table:dm-known-optimizations} summarizes the optimizations done to the know  devices workload. 
As before we can seen different implementations prefixes (dm-0,dm-1,...dm-4) that are described in Table~\ref{table:names:dm}. 
From left to right we see improvement in the workloads. 

First  (dm-1) the two-step operation required to fetch a device is replaced by a single read  with the help of Cross References tables (just as we have done in the analogous Load Balancer workload in Table~\ref{table:lbw:optimizations}). 
Next, we upgrade  the replace operation used to update the Device timestamp in dm-0 by a versioned based replace. 

Following it, in dm-3 we use a Column table to store devices.  
However,  we nearly do not improve  while fetching the source device since the local logic requires reading almost all the device attributes. 
On the other hand, the update timestamp operation drops to half the request size when compared to the previous.  
This improvement is caused by using only the updated column in the replace operation. 
We could improve this even further if we kept the timestamp in a single column on the data store. 
However, our current data store prototype cannot easily break each array element in a different column. 
Finally,  with columns we can reduce the return size of the last operation by a factor of 3 since we only need to read the switch attachment points of the destination device. 

For the final improvement, we use micro-components (lbw-4).  Two components are created for the known device workload. 
First, one that merged the two operations required to fetch the source and destination device in only one operation.
Second, one to update the timestamp in the data store.  In this operation,  only the device key and the new timestamp are sent. This  significantly improves the message size (notice that a similar effect could be obtained with a more powerful column data model).  

However,  the most significant improvement of micro components is for creating devices. Table~\ref{table:dm-unknown-optimizations} shows that all different implementations of the Device Manager applications are nearly not affected by the improvements  done up to lbw-3 (column based implementation). However, the introduction of a micro component to create a device replaced  4 operations (from \#2 to \#5) with a single one.  In addition, the source and destination device are read simultaneously just as in the known-device workload. 

The reader may be asking why not merge all the operations (reading the device and updating or creating it) in a single powerful 
micro-component? However, being that other controller applications have to consulted prior to a device creation this is impossible. 

Our results confirm \ref{fig:dm:performance} that the most significant improvement comes from creating a micro component to create a device. With it we reduce a latency penalty superior to 10 ms to under 4 ms while elevating the data store processing rate from 2kFlows/s to nearly 12kFlows/s.  For the well-known device workload our measurements show a similar result. 

\subsection{Cache}
As before, we can introduce the cache to serve a portion of the reads locally. 
This time we keep the \texttt{devices} in cache. 
Again, the same logic  can be applied: we can use the write present in our workload to verify if our current data is stale or not.  Then in the known devices workload case when we update the timestamp with the local version number of the device (present in cache), the operation will only succeed if we have the most recent device version. 
The same logic is followed in the unknown device case but this time with the micro-component to create a device. 
We expect this operation to be successful a sufficient number of times to justify the use of cache since we do not anticipate frequent scenarios where more than one controller (or even different threads in same one) update the same device. 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../PEI"
%%% End: 

