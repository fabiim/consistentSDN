%!TEX root = ../PEI.tex
\label{sec:feasibility:apps}
\glsresetall

We are considering applications in isolation. DeviceManager could have
applications registered on device events (updates, deletion etc.,)
that would cause more traffic to the data store. 
 


Mininet  Hi-Fi \cite{Handigol:2012tg}\footnote{Version 2.0 (mininet-2.0.0-113012-amd64-ovf) available at \url{http://mininet.org}}. Unfortunely we had to hack the box configuration to avoid that hosts  triggered gratioutious data packets that would  trigger traffic to the controller when initializing the switch-controller connection. We changed the \texttt{net\.ipv6\.conf\.all\.disable\_ipv6,net\.ipv6\.conf\.default\.disable\_ipv6} fields in the file \texttt{/etc/sysctl\.conf} to true\footnote{This is a known problem https://mailman.stanford.edu/pipermail/mininet-discuss/2010-November/000167.html (contains solution)}. 

We do not log creation, and deletion of tables. Those will not appear in the workloads. This is nor relevant since it usually happens upon a first switch connection or first host packet processed by an application  for that switch


\section{Learning Switch}
\label{sec:feasibility:ls}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo{IT IS NOT IP. IT IS A MAC.}
\todo{Learning switch is what Kandoo calls a local app}
\begin{figure}[ht]

  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-broadcast}
                \caption{Broadcast packet.}
                \label{fig:ls:interaction:broadcast}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Unicast packet.}
                \label{fig:ls:interaction:unicast}
        \end{subfigure}
        \caption[Learning Switch workloads]{Broadcast packets trigger a write for the source address of the respective packet. Unicast packets have to additionally read the source address port location.}
        \label{fig:ls:interaction}
\end{figure}

The Learning Switch application emulates the hardware layer 2 switch
forwarding process. For each switch a different \emph{\texttt{MAC}-to-switch-port}
table is maintained in the data store. Each table is populated using
the source address information (i.e., \texttt{MAC} and port)  present in every OpenFlow
\texttt{packet-in} request for the purpose of maintaining the location
of devices. After learning this location, the controller can install
rules in the switches to forward packets from a source to a
destination. Until then, the controller must instruct the switch to
\emph{flood} the packet to every port, with the exception of
  the ingress port. Despite being distributed, each switch-table is
  only accessed by the controller managing the switch in
  question. Even so, we justify the study  of this application for two
  reasons: (i) it benefits from the fault-tolerance property of
  our distribution and (ii) it is commonly used as the
  single-controller benchmark application in the literature \cite{Tootoonchian:2012uia}. 

\todo{to be common you need more than 1 example}

Figure \ref{fig:ls:interaction}  shows the detailed interaction between the
switch, controller (Learning Switch) and data store for two possible
cases. First (figure \ref{fig:ls:interaction:broadcast}), the case for broadcast packets that require
one write operation to store the switch-port  of the
source address. Second (figure \ref{fig:ls:interaction:unicast}),   the case for unicast
packets, that not only stores source information, but also read the
switch egress port for the destination address.  


\note{Nao esta  claro que a tabela nesta app e single
  reader, single writer. NInguem entendeu...}

Original:  Map<IOFSwitch, Map<MacVlanPair,Short>> . Modified : Map<IOFSwitch, Map<MacVlanPair,Short> . Originally instanced with concurrent hashmaps for the main map and each table map. 
CHANGE: We still maintain a concurrentMap for the main switch-to-KeyValueTable map. This is required since the manipulation of the map is done concurrenty by: each switch thread and  by the REST API.
There isn't any problem since KeyValueTable is  thread safe \ref{section.datastore.thread.safety}

Original: It is critical that the size of each switch table is limited 
due to resource usage.  Mac addresses have to be recycled as devices enter and leave the network. Or simple as traffic dynamics change. Either way in some networks we can't possibly support all the hosts \ref{sec.learning.switch.study.size} for table. Remember each table can contain all hosts in the network! Traditionally the LearningSwitch uses a LRULinkedHashMap (Least Recently Used Linked Hash Map). This imposes a limit on the number of entries present in the table. The behaviour of this LRULInkedHashMap is to remove the oldest entry in the table whenever the threshold is reached.  The default threshold (used in all our experiences unless we state otherwise) is of 1000 hosts per table. (Remember that each table can potentially keep an entry for each host in the all network! ). 
Change:  We replicate this kind of table in the data store. Which was fairly simple. Off course the behaviour changes in the case we use a cache. As discussed in section \ref{sec.learning.switch.lru.cache}

The controller only replies to one switch.  This causes the problem set by \cite{of.cpp} of mac broadcasting REVISE EXAMPLE.  The path is set switch by switch. 

The application is configured with a idle timeout of 5 and a hard timeout of 0 seconds. As such the switch deletes the entry after 5 seconds of inactivity. This will result in a delete from the switch table that will trigger an OpenFlow \texttt{FLOW\_REMOVED } message from the switch to the controller. The Learning switch applications will process this OF message and delete the corresponding entry in the data store. Then, (immediately after) it will instruct the switch the remove the reverse entry from its table. The switch upon processing this message will trigger another \texttt{FLOW\_REMOVED} message to the controller.  This recursive process is not infinitive. The switch will only trigger \texttt{FLOW\_REMOVED} messages when it deletes an entry from the table. If the controller instructs it to remove something that isn't there, we will not trigger any message (sec. 6.4 of \cite{openflow-spec}). 


\subsection{Broadcast Packet}
This workload corresponds to the operations performed in the data
store when processing broadcast packets in a OpenFlow
\texttt{packet-in} request.  Table \ref{table:lsw0:broadcast} shows that for the
purpose of associating the source address of the packet to the ingress
switch-port where it was received, the Learning Switch application performs one
write operation with a request size of 113 bytes and reply size of 1
byte. 

\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
 Associate source address to ingress port & W & 113 & 1 \\ \bottomrule
\end{tabular}
\caption[Workload lsw-0-broadcast( Broadcast Packet) operations]{Workload lsw-0-broadcast( Broadcast Packet) operations and sizes (in bytes).}
\label{table:lsw0:broadcast}
\end{table}

\subsection{Unicast Packet}
Workload \textbf{lsw1-1} is the result of processing an OpenFlow request
triggered by an unicast packet. Thus,  when compared to the previous
workload (\textbf{lsw1-0} covering broadcast packets), an additional
operation is required to query the switch-port location of the
destination address. Table \ref{table:lsw0:unicast} provides a summary all the data
store operations in this workload. 

\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply  \\ \toprule 
 Associate source address to ingress port & W & 113 & 1\\\midrule
Read egress port for destination address & R & 36 & 77 \\\bottomrule
\end{tabular}
\caption[Workload lsw-0-unicast( Unicast Packet) operations]{Workload lsw-0-unicast( Unicast Packet) operations and sizes (in bytes).}
\label{table:lsw0:unicast}
\end{table}

\subsection{Optimizations}
Our serialization process between ewsdn and now has changed. In ewsdn we used longer tablenames and java serialization. Aftewards we changed our codes so that each tablename is the concatenation of ``LS''  and the switch identifier (which can be as long as FIXME characters.  Also we have manually convert the information stored in the data store. Notice for example that between the examples the read egress port operation, the return value (a switch port idenfifiers) varies is 12 times greater in the java serialization process. 


\begin{table}[ht]
\centering 
\begin{tabular}{l c c c c}
 Operation & Type & Request & Reply \\ \toprule 
Associate source address to ingress port & W & 29 & 1\\\midrule
Read egress port for destination address & R & 27 & 6 \\\bottomrule
\end{tabular}
\caption[Workload lsw-1-unicast( Unicast Packet) operations]{Workload lsw-1-unicast( Unicast Packet) operations and sizes (in bytes).}
\label{table:lsw1:unicast}
\end{table}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{../data/reportGenerator/lsw-0-broadcastlsw-0-unicastlsw-1-broadcastlsw-1-unicasttxLatCmp.pdf}
\caption[Learning Switch workloads performance comparison]{Learning Switch workloads performance comparison (90th percentile). }
\end{figure}
\subsection{Cache}

\label{sec.learning.switch.lru.cache} Discuss cache implications of
least recently used. 

We only get values from cache if they exist despite the time they are
present there. This happens since Learning Switch installs rules with
a idle timeout. So when rules expire, the switch warns the Learning
Switch application which in turn deletes the entries in the data
store. So we don't actually require to be worried about stale values
in the cache being invalid (e.g., a host moved from one port to
another). If they are, the idle mechanism associated with rules
installed in the data plane will, eventually, correct the situation.
Off course we are not strongly consistent anymore. 

We don't actually improve on the micro-benchmarks tested measures
shown throughout this chapters. We do not improve simply because with
cache we do not avoid or improve any of the data store interactions
present in table \ref{table:work:lsw1-1} (that shows the latest
learning switch workload).  With cache we will only improve on the
long run, since we can now avoid the two type of requests present in
that table. First we can avoid re-writing the source address to source
port association when we already now it. In the original learning
switch association this is re-write is not costly ($\Omega(1)$ ) and also
has the functional impact of refreshing the entry timestamp such that
the least recently used table can keep up consistently with the last
active hosts and delete the not active ones. In the cache
implementation this is not true anymore. Now, the active hosts
actually get forgotten somewhere in time as newly (unknown) entries
are added to the data store. This is not problematic since the host,
being active, will benefit from latency a lot before actually being
erased from the  the data store due to the newly added hosts. This off
course is explicitly dependant of the maximum number of entries per
table. 
\footnote{\url{http://docs.oracle.com/javase/6/docs/api/java/util/LinkedHashMap.html}}

(Note that insertion order is not affected if a key is re-inserted
into the map. (A key k is reinserted into a map m if m.put(k, v) is
invoked when m.containsKey(k) would return true immediately prior to
the invocation.)).  So NOTHING I SAID WAS TRUE! :) BUT IT APPLIES TO
GETS : In access-ordered linked hash maps, merely querying the map
with get is a structural modification.)
FIXME : YOU MUST ACTUALLY CONFIRM ALL THIS. AND ADD SUPPORT IN THE
IMPLEMENTATION OF THE LEARNING SWITCH. 

When avoiding this write
in the cache implementation we must actually be sure that we only
avoid to write to the data store when the association is known in
cache and it is actually correct (the ingress port is the same from
the packet being processed). 

The second avoidance is the read operation that queries for the egress
port of the current processed packet. We do not actually need to read
from the data store it the entry is present in the cache. First the
data is not been modified by any other controller since we are the only
ones which manipulate our switch tables. 


With cache we no longer read from the database. We do not need since
put updates the cache. So if it isn't on the cache it is not in the
data store.



% \begin{figure}
% \centering
% \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
% \caption[]{\textbf{}}
% \end{figure}

\label{cenas}

% \begin{figure}
% \centering
% \includegraphics[width=\textwidth]{pic/feasibility/ls-events-broadcast}
% \caption[]{\textbf{}}
% \end{figure}

\section{Load Balancer}
\label{sec:feasibility:lb}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Load Balancer application employs a round-robin algorithm to distribute the
requests addressed to a virtual endpoint \texttt{IP} (VIP). In order to
understand its behaviour we begin by the data model currently used. Figure
\ref{fig:lb-model} shows the three different entities used in the Load
Balancer. The  VIP entity 
represents a virtual endpoint with a specified \texttt{IP}, port and
protocol address. Each VIP can be associated with one or more pools of
servers. Given that the distribution algorithm is round-robin, each pool
has a current assigned server (\texttt{current-member} attribute in the figure). Finally, the third entity --- Member
--- represents a real server. Each of those entities, corresponds
to a different table in the data store, indexed by the entity
key attributes represented in the figure (in bold). Moreover, a fourth table is
required to associate \texttt{IP} addresses to VIP resources. In light of
this data model, the load balancer logic requires the following
operations from the data store: (i) check if the source address is
associated with a VIP resource; (ii) if so, read the VIP, Pool and
Member information required to install flows in the switch and (iii)
update the pool \texttt{current-member} attribute. This description corresponds to the case where OpenFlow
\texttt{packet-in} requests are indeed addressed at a VIP
resource. The respective workload which, \textbf{markedly - replace,
  aly says is wrong TODO} is the heavier in
the Load Balancer application, is described in detail in the workload
\textbf{lbw1-0} section ahead. Alternatively, workload
\textbf{lbw1-1} captures the workload created by every
packet unrelated to a VIP. Finally, workload \textbf{lbw1-2}
considers the special case of ARP requests questioning the hardware
address of a VIP \texttt{IP}.

\begin{figure}[H]
\centering 
\includegraphics[scale=0.5]{./lb-model.pdf}
\caption{\small Simplified Load Balancer entities data model. The data
store contains a table for each entity, indexed by their keys (represent as bold attributes). }
\label{fig:lb-model}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-broadcast}
                \caption{ARP packet address at a VIP.}
                \label{fig:lb:interaction:arp2Vip}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/lb-events-unicast}
                \caption{}
                \label{fig:lb:interaction:ip2Vip}
        \end{subfigure}
        \caption[Load Balancer workload events]{A \texttt{\gls{arp}} request message addressed at a VIP \gls{ip} that results in a direct \gls{arp} reply. On the left a normal \gls{ip} packet addressed at VIP should be resolved (who is responsible) and replied by installing the appropriate rules}  
        \label{fig:lb:interaction}
\end{figure}

\subsection{Packets to a VIP}
When the Load Balancer  receives a data packet addressed
at a VIP, it triggers the operations seen in table
\ref{table:workload:lbw1-0}. 
The first operation fetches a VIP resource associated with the
destination \texttt{IP} address of the packet.
If it succeeds (reply different from 0), then it proceeds to read 
the chosen pool for the returned  VIP\footnote{The current implementation of this
application always chooses the first existent pool.}.
Afterwards it updates (third operation) the fetched  pool, along with the newly modified
\texttt{current-member}.
The forth and final operation retrieves
the address information for the selected  Member. 

\note{ discutir a questão de update concorrente (segunda e  terceira operacao )}

\subsection{Normal Packets}
Even when processing a normal packet, not related to a VIP address at
all, the Load Balancer still has to find out if this is the case. This workload, which only requires one operation (see
table \ref{table:workload:lbw1-1}) sets the minimum amount of work imposed by
the Load Balancer to the controller pipeline. 

\subsection{ARP Request}
This workload  results  from processing an ARP Request addressed at a
VIP address. The data store operations, summarized in Table
\ref{table:workload:lbw1-2}, shows that two reads are
required. First, as previously seen,  it queries the data
store to check if the packet destination address is a VIP (1 read
needed). As it is, the controller then retrieves the \texttt{MAC} address for that
VIP server (so, another read is needed).


\section{Device Manager}
\label{sec:feasibility:dm}
\glsresetall
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/dm-unknown}
                \caption{Packet from an unknown device.}
                \label{fig:dm:interaction:unknown}
        \end{subfigure}%
        ~
        \begin{subfigure}[b]{0.5\textwidth}
                \centering
                \includegraphics[width=\textwidth]{pic/feasibility/ls-events-unicast}
                \caption{Packet from a known device.}
                \label{fig:dm:interaction:known}
        \end{subfigure}
        \caption[Device Manager workload events]{Workloads for this application heavily depend on the state of the data store. Unknown devices trigger several operations to the creation of these, while known devices only require an update of their "last seen" timestamp. No matter the case, the source and destination devices are retrieved if they exist.}
        \label{fig:dm:interaction}
\end{figure}



