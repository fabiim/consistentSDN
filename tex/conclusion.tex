\section{Conclusions}
\todo{Por escrever...}
\subsection{Frow Ewsdn}
In this thesis we have proposed a distributed, highly-available, strongly consistent controller for SDNs.
The central element of the architecture is a fault-tolerant data store that guarantees acceptable performance.
We have studied the feasibility of this distributed controller by analyzing  the workloads generated by representative SDN applications and demonstrating that the data store is capable of handling these workloads, not becoming a system bottleneck.

The introduction of a fault-tolerant, consistent data store in the architecture of a distributed SDN controller has a cost.
Adding fault tolerance increases the robustness of the system, while strong consistency facilitates application design, but the fact is that these mechanisms affect system performance.
First, the overall throughput will decrease to the least common denominator, which will in most settings be the data store.
Second, the total latency will increase as the response time for a data path request now has to include i) the latency to send a request to the data store; ii) the time to process the request; and iii) the latency to reply back to the controller.
Starting by assuming the inevitability of this cost, our objective in this thesis is to show that, for some network applications at least, the cost may be bearable and the overall performance of the system remain acceptable.

The drawback of a strongly consistent, fault-tolerant approach for an SDN platform is the increase in latency, which limits responsiveness; and the decrease in throughput that hinders scalability.
Even assuming these negative consequences, an important conclusion of this study is that it is possible to achieve those goals while maintaining the performance penalty at an acceptable level.

As the number of SDN production networks increase the need for dependability becomes essential. The key takeover of this work is that dependability mechanisms have their cost, and it is therefore an interesting challenge for the SDN community to integrate these mechanisms into scalable control platforms. 

\subsection{Things we found out}
\begin{itemize}
\item Scalability, reactive model, real-world-applications: 
  \begin{itemize}
  \item The partition of the data plane across different controllers should enable scalabality. However, in our experience each event triggered by the switches triggers at least a write (in most cases) to the data store. This may hinder scalability of the control plane. 
  \item It can be that our architecture is not a best-fit for the reactive model. 
  \item However we did not take an approach of modifying  the semantic of the applications! It can be that modifed applications can behave better. (Device Manager and timestamp).  
  \end{itemize}
\end{itemize}

\subsection{Limitations}
\begin{itemize}
\item Our analysis is not thorough  because it does not accounts for the other applications effects (e.g., device manager may consult the topology manager); 
% \item It is important to clarify from the outset that the feasibility study analysis is arguably simplistic since it is focused in on data plane plane events in isolation . Considered in isolation, 
% other applications behaviour can impact on the workloads  by communicating with the application API.
% \item Networks are self-healing by nature. The use of a distributed control plane with solid (consistent)  seems to benefit optimiality and simplicity, but does not seems to solve much of the problems present in the data plane. 
\item We do not have a real data plane workload. What kind of events are more frequent? It can have a deep impact;
\item It is only a micro-benchmark, the controller performance must be taken into consideration; 
\item We need Query Languages. Reading entire tables is proibitive as we personal have seen in the Topology manager application. 
%Read entire tables, a common behaviour observed has not practical solution.  This will off course result in a giant overhead to the data store. Naturally this can be solved by introducing stale data with a cache, which may not be harmfull if the applications are not in a critical path to the network control goals (e.g., systems who display information). However we have seen this kind of code in Floodlight applications. In fact, there was one application that was left out of this work in a early stage, since after we adapt it to our data store and analyzed its behaviour we found that on each topological change to the network, it required reading all the data store state to behave 
\end{itemize}


% \begin{itemize}
% \item The application behaviour found in the controller is too demaning for consistent data stores. And it can not even justify the usage of this data stores. 
% \item An example is the Device Manager which updates the last seen timestamp on every flow arriving at the controller. If this value is not careful choosen it may impose an extreme demand to the data store
% \item Another example is the Load Balancer. Even it Levin et. al suggest that a stale view of the network incurrs in beneath optimal (when compared to a consistent view) balancing performance the behaviour that we encounter in the Load Balancer application definitely limits the controller the controller scalability severely when contrasted with the single controller in multi-cores. 
% \item This issues can be addressed with two events: either the data store supports a more scalable consistency model or the applications have to be modified to be more pro-active. By pro-active we mean that hey must stop resorting to flow requests sent to the controller to maintain an updated network view. We reason that more efficient methods can be built that can be as much flexible as the existing ones and also more efficient given an controller with our characteristics. For example the Device Manager should resort to additional  events instead of flow requests to find out mobility, while the forwarding applications could install flows that expire on activity based rule space (there are works that adress the switch table problems without resorting to least recently used like cache mechanisms). This way the switch input rate imposed on the controller can be lowered possibly in ways that can be satisfied by our current data store performance. Also the Load Balancer could resort to periodically consult the switch channels condition (usage) and update the data store. By batching the entire switch domain conditions we could have a fined grained  view of the entire network state kept consistently in the data store. The controller can even choose to have different window of inconsistencies values for different subsets of the data plane according to the importance of the Load Balancer activity. 
% \end{itemize}


% \begin{itemize}
% \item So in the best case we have 200kFlows/s with a data store.  That 2 out 3 applications that are fundamental in the packet in pipeline require at least one write operation exchanged with the data store. So even with top-notch engineering techniques of caching, batching, non-blocking I/O , we are definitely  setting a hard limit on the controller scalability. In fact this drop the processing rate of state of the art centralized controllers by a factor of 5 (at  least!). 
% \item We distribute a control plane for three reasons: fault tolerante (reliability!), scalability and performance (in the controller case, this concerns the latency between switches and the controller). 


\subsection{Future Work}
\begin{itemize}
\item Add applications that we believe to be fundamental to the data center: Topology Manager and Firewall.  Study the pipeline effect of all the applications together and improve on that. 
\item The remaining architecture. 
\item Multiple consistency semantics, bessani protocol, relax on reads operations. 
\item ``Transactional Pipeline'', a single event, multiple applications, a single round trip to the data store. 
\item Explore the proactive model. It is a better fit for our architecture if scalability is an issue. forwarding requests are just too much. Network changes are easily dealt with.... 

% \item Study the differences between serializability and strong consistency, establishing when do the applications require one or another. State machine replication is not required in all aplications, use strong consistency in some cases (reads, writes). Bessani Protocol, cassandra with strong consistency scales better than SMR , etc., 
\end{itemize}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../PEI"
%%% End: 
