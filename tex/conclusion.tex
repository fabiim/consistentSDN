\section{Discussion}
\subsection{Centralized Controllers}

First, we note that the performance results of our data store are similar to those reported for the original NOX and other popular SDN controller\,\cite{Tootoonchian:2012:CPS:2228283.2228297}.
The average throughput for the Learning Switch application (the only application considered in\,\cite{Tootoonchian:2012:CPS:2228283.2228297}) is not far from that reported by NOX (30kReq/s), so our data store would not become a bottleneck in this respect.
In addition, the latency is close to the values reported for the different SDN controllers analyzed in that work (including the high-performance, multi-threaded ones), so the additional latency introduced, although non-negligible, can (arguably) be considered acceptable.
We consider this result to be remarkable given that our data store provides both strong consistency and fault tolerance.

Of course, the insightful reader will note that the results become quite distant from what is obtained with a controller that is optimized for performance, such as NOX-MT~\cite{Tootoonchian:2012:CPS:2228283.2228297}, particularly in terms of throughput.
As the second part of the argument, it is important to understand that every update to our data store represents an execution of the protocol of Fig. \ref{fig:paxos}, while in NOX-MT we have simply OF requests being received by a controller with the data store kept in main memory.
Even if NOX-MT (or any other high-performance controller) synchronously writes particular data to disk (something that takes around 5ms), no more than 200 updates/second can be executed.
This unequivocally shows that if some basic durability guarantees are required (e.g., to ensure recoverability after a crash), then the impressive capabilities of these high-performance controllers will be of little use.

\subsection{Distributed Controllers}

1) que não há detalhe suficiente no artigo do Onix sobre a datastore
consistente deles
2) mas que pela má performance podemos conjecturar que eles não estão a usar
as técnicas state-of-the-art que estão a ser usadas naqueles exemplos que
damos (Spanner, etc.)
3) por isso o nosso objetivo é elevar a performance usando estas técnicas
mais recentes
4) e acho que valeria a pena, talvez quando se fala do bft-smart e da
performance dar o exemplo de uma ou duas das tais técnicas state-of-the-art
que permitem uma performance melhor. (Pode adicionar-se uma referência para
um artigo onde se fale da performance do bft-smart?)


\subsection{Consistent Data Plane}

\section{Future Work}
% \begin{itemize}
% \item So in the best case we have 200kFlows/s with a data store.  That 2 out 3 applications that are fundamental in the packet in pipeline require at least one write operation exchanged with the data store. So even with top-notch engineering techniques of caching, batching, non-blocking I/O , we are definitely  setting a hard limit on the controller scalability. In fact this drop the processing rate of state of the art centralized controllers by a factor of 5 (at  least!). 
% \item We distribute a control plane for three reasons: fault tolerante (reliability!), scalability and performance (in the controller case, this concerns the latency between switches and the controller). 


\subsection{Future Work}
\begin{itemize}
\item Add applications that we believe to be fundamental to the data center: Topology Manager and Firewall.  Study the pipeline effect of all the applications together and improve on that. 
\item Study the differences between serializability and strong consistency, establishing when do the applications require one or another. State machine replication is not required in all aplications, use strong consistency in some cases (reads, writes). Bessani Protocol, cassandra with strong consistency scales better than SMR , etc., 
\end{itemize}


\section{Conclusions}

In this paper we have proposed a distributed, highly-available, strongly consistent controller for SDNs.
The central element of the architecture is a fault-tolerant data store that guarantees acceptable performance.
We have studied the feasibility of this distributed controller by analyzing  the workloads generated by representative SDN applications and demonstrating that the data store is capable of handling these workloads, not becoming a system bottleneck.

The introduction of a fault-tolerant, consistent data store in the architecture of a distributed SDN controller has a cost.
Adding fault tolerance increases the robustness of the system, while strong consistency facilitates application design, but the fact is that these mechanisms affect system performance.
First, the overall throughput will decrease to the least common denominator, which will in most settings be the data store.
Second, the total latency will increase as the response time for a data path request now has to include i) the latency to send a request to the data store; ii) the time to process the request; and iii) the latency to reply back to the controller.
Starting by assuming the inevitability of this cost, our objective in this paper is to show that, for some network applications at least, the cost may be bearable and the overall performance of the system remain acceptable.


The drawback of a strongly consistent, fault-tolerant approach for an SDN platform is the increase in latency, which limits responsiveness; and the decrease in throughput that hinders scalability.
Even assuming these negative consequences, an important conclusion of this study is that it is possible to achieve those goals while maintaining the performance penalty at an acceptable level.

As the number of SDN production networks increase the need for dependability becomes essential. The key takeover of this work is that dependability mechanisms have their cost, and it is therefore an interesting challenge for the SDN community to integrate these mechanisms into scalable control platforms. 

\subsection{Limitations}
\begin{itemize}
\item Read entire tables, a common behaviour observed has not practical solution.  This will off course result in a giant overhead to the data store. Naturally this can be solved by introducing stale data with a cache, which may not be harmfull if the applications are not in a critical path to the network control goals (e.g., systems who display information). However we have seen this kind of code in Floodlight applications. In fact, there was one application that was left out of this work in a early stage, since after we adapt it to our data store and analyzed its behaviour we found that on each topological change to the network, it required reading all the data store state to behave 
\item It is important to clarify from the outset that the feasibility study analysis is arguably simplistic since it is focused in on data plane plane events in isolation . Considered in isolation,
other applications behaviour can impact on the workloads  by communicating with the application API.

\item Networks are self-healing by nature. The use of a distributed control plane with solid (consistent)  seems to benefit optimiality and simplicity, but does not seems to solve much of the problems present in the data plane. 
\end{itemize}


\begin{itemize}
\item The application behaviour found in the controller is too demaning for consistent data stores. And it can not even justify the usage of this data stores. 
\item An example is the Device Manager which updates the last seen timestamp on every flow arriving at the controller. If this value is not careful choosen it may impose an extreme demand to the data store
\item Another example is the Load Balancer. Even it Levin et. al suggest that a stale view of the network incurrs in beneath optimal (when compared to a consistent view) balancing performance the behaviour that we encounter in the Load Balancer application definitely limits the controller the controller scalability severely when contrasted with the single controller in multi-cores. 
\item This issues can be addressed with two events: either the data store supports a more scalable consistency model or the applications have to be modified to be more pro-active. By pro-active we mean that hey must stop resorting to flow requests sent to the controller to maintain an updated network view. We reason that more efficient methods can be built that can be as much flexible as the existing ones and also more efficient given an controller with our characteristics. For example the Device Manager should resort to additional  events instead of flow requests to find out mobility, while the forwarding applications could install flows that expire on activity based rule space (there are works that adress the switch table problems without resorting to least recently used like cache mechanisms). This way the switch input rate imposed on the controller can be lowered possibly in ways that can be satisfied by our current data store performance. Also the Load Balancer could resort to periodically consult the switch channels condition (usage) and update the data store. By batching the entire switch domain conditions we could have a fined grained  view of the entire network state kept consistently in the data store. The controller can even choose to have different window of inconsistencies values for different subsets of the data plane according to the importance of the Load Balancer activity. 
\end{itemize}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../PEI"
%%% End: 
